<!DOCTYPE html>
<html lang="ru" data-vue-meta="%7B%22lang%22:%7B%22ssr%22:%22ru%22%7D%7D">
<head >
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,viewport-fit=cover">
  <title>Вижу, значит существую: обзор Deep Learning в Computer Vision (часть 1) / Хабр</title>
  <style>
    /* cyrillic-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSxf6TF0.woff2) format('woff2');
      unicode-range: U+0460-052F, U+1C80-1C88, U+20B4, U+2DE0-2DFF, U+A640-A69F, U+FE2E-FE2F;
    }

    /* cyrillic */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveQhf6TF0.woff2) format('woff2');
      unicode-range: U+0400-045F, U+0490-0491, U+04B0-04B1, U+2116;
    }

    /* latin-ext */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveSBf6TF0.woff2) format('woff2');
      unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
    }

    /* latin */
    @font-face {
      font-family: 'Fira Sans';
      font-style: normal;
      font-weight: 500;
      font-display: swap;
      src: url(https://fonts.gstatic.com/s/firasans/v11/va9B4kDNxMZdWfMOD5VnZKveRhf6.woff2) format('woff2');
      unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
    }
  </style>
  <link rel="preload" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" as="script"><link rel="preload" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css" as="style"><link rel="preload" href="https://assets.habr.com/habr-web/js/app.c0af73e7.js" as="script">
  <link rel="stylesheet" href="https://assets.habr.com/habr-web/css/chunk-vendors.e7843cc0.css"><link rel="stylesheet" href="https://assets.habr.com/habr-web/css/app.02ee25a4.css">
  <script>window.i18nFetch = new Promise((res, rej) => {
          const xhr = new XMLHttpRequest();
          xhr.open('GET', '/js/i18n/ru-compiled.85eb77f0b17c8235e7b64b9f81ea5ec2.json');
          xhr.responseType = 'json';
          xhr.onload = function(e) {
            if (this.status === 200) {
              res({ru: xhr.response});
            } else {
              rej(e);
            }
          };
          xhr.send();
        });</script>
  
  <script data-vue-meta="ssr" src="/js/ads.js" onload="window['zhY4i4nJ9K'] = true" data-vmid="checkad"></script><script data-vue-meta="ssr" type="application/ld+json" data-vmid="ldjson-schema">{"@context":"http:\/\/schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/habr.com\/ru\/company\/mipt\/blog\/450732\/"},"headline":"Вижу, значит существую: обзор Deep Learning в Computer Vision (часть 1)","datePublished":"2019-05-21T14:31:04+03:00","dateModified":"2020-01-03T22:12:43+03:00","author":{"@type":"Person","name":"Илья Захаркин"},"publisher":{"@type":"Organization","name":"Habr","logo":{"@type":"ImageObject","url":"https:\/\/habrastorage.org\/webt\/a_\/lk\/9m\/a_lk9mjkccjox-zccjrpfolmkmq.png"}},"description":"Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примера...","url":"https:\/\/habr.com\/ru\/company\/mipt\/blog\/450732\/#post-content-body","about":["c_mipt","h_programming","h_image_processing","h_machine_learning","h_artificial_intelligence","f_develop","f_popsci"],"image":["https:\/\/habrastorage.org\/getpro\/habr\/post_images\/ecb\/319\/e06\/ecb319e06d692a5ea4f2a1343cf9c31d.jpg","https:\/\/habrastorage.org\/webt\/gu\/vu\/o3\/guvuo3vejwwjimlpcqiwgbpxldq.jpeg","https:\/\/habrastorage.org\/webt\/3x\/tl\/-j\/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/967\/987\/50c\/96798750c04282d6514f994b8375edcb.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/dda\/997\/082\/dda9970829bfb17bb2b118a08d519835.jpg","https:\/\/habrastorage.org\/webt\/jz\/9k\/2o\/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/4d1\/fb8\/125\/4d1fb8125d4624b40993f441b42ac48d.jpg","https:\/\/habrastorage.org\/webt\/wf\/kw\/la\/wfkwlap8pltophsuh1ggkxgkii8.jpeg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/2d3\/f3b\/178\/2d3f3b17818ae279e7a47d3c940e002f.jpg","https:\/\/habrastorage.org\/webt\/hn\/cw\/oc\/hncwocoggiei8lkijpl8ihgbx_o.jpeg","https:\/\/habrastorage.org\/webt\/vg\/vv\/4f\/vgvv4f_ddwswudk1yvghxjl4rne.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/b4d\/cfd\/d13\/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/cfa\/2bb\/afa\/cfa2bbafae96a5bd082ef25bae9d19af.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/60d\/62d\/670\/60d62d670999dcc7cbd726dde47905a0.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/113\/220\/ca0\/113220ca03176c5a99b82819076e0c8a.jpg","https:\/\/habrastorage.org\/webt\/ju\/b7\/i6\/jub7i61z3oiairdg2q45x0l6loi.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/d56\/155\/0ee\/d561550eec9f5badc4475392a584fe03.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/859\/ffd\/2d5\/859ffd2d56f231c5f9b802978a688c94.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/31f\/003\/a47\/31f003a47c5dc5b5c5f75758d4d3689c.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/9a4\/3ea\/74b\/9a43ea74ba0b5595f257feb313756293.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/e90\/962\/25b\/e9096225bb7d5799823737c960e19ad6.jpg","https:\/\/ai2-s2-public.s3.amazonaws.com\/figures\/2017-08-08\/38211dc39e41273c0007889202c69f841e02248a\/2-Figure1-1.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/074\/e15\/f04\/074e15f04c8347ab32f98ba04aeceb6c.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/26c\/167\/e3f\/26c167e3feb823e778b32278358053f9.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/fb1\/3ca\/d97\/fb13cad97db640053bb2c53c12b0f4a7.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/c28\/ab9\/3c6\/c28ab93c670c1e44258dc86064bb3a0c.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/bd4\/27e\/5e2\/bd427e5e2943ebf58409e42538c4e131.png","https:\/\/habrastorage.org\/webt\/z3\/i4\/b4\/z3i4b4pxfnulxzfszysn_usqn_c.png","https:\/\/habrastorage.org\/webt\/fz\/vm\/ym\/fzvmymab57wgircssyfgxiaomvy.jpeg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/e55\/809\/63f\/e5580963fdfb998bfe2103f4cbf5aa8c.jpg","https:\/\/habrastorage.org\/webt\/b-\/-k\/iw\/b--kiw7-vibdk8vpuzfxhbagkuu.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/5cd\/b10\/ea8\/5cdb10ea8f19fe29432265e906640a90.jpg","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/887\/d76\/eb4\/887d76eb431bcaf434ff70e2e0f2d4b0.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/55d\/ca5\/358\/55dca535836121c65546bc11e2d457c1.png","http:\/\/api.ning.com\/files\/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg\/transferlearning.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/fbc\/3ad\/f28\/fbc3adf280e28f7bb71246f50c1e8d9e.jpg","https:\/\/habrastorage.org\/webt\/jh\/43\/xs\/jh43xsjjgxixbw8cmo1idxage5a.jpeg","https:\/\/habrastorage.org\/webt\/9s\/pj\/cm\/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg","https:\/\/habrastorage.org\/webt\/16\/uh\/n8\/16uhn8l_iahuy-bcv_e4vohx1je.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/208\/1b9\/d34\/2081b9d346f74503302b8fd2c7265ef5.png","https:\/\/habrastorage.org\/webt\/t9\/k3\/zv\/t9k3zvmuf30yzmcvlube_okh5ey.png","https:\/\/habrastorage.org\/webt\/f_\/pe\/cd\/f_pecd2dvv5kbatdpj0nkk4aapm.png","https:\/\/habrastorage.org\/webt\/ca\/pn\/gr\/capngrtiskbeltdx0oq_wfntw0i.png","https:\/\/habrastorage.org\/getpro\/habr\/post_images\/456\/66e\/9c0\/45666e9c0608373c31452aeb6a197477.jpg","https:\/\/habrastorage.org\/webt\/ue\/22\/e1\/ue22e11md3zjexlxq3jxsf-kx18.jpeg"]}</script>
  <script src="//www.googletagservices.com/tag/js/gpt.js" async></script>
  <style>.grecaptcha-badge{visibility: hidden;}</style>
  <meta name="habr-version" content="2.49.0">
  
  <meta data-vue-meta="ssr" property="fb:app_id" content="444736788986613"><meta data-vue-meta="ssr" property="fb:pages" content="472597926099084"><meta data-vue-meta="ssr" name="twitter:card" content="summary_large_image"><meta data-vue-meta="ssr" name="twitter:site" content="@habr_eng"><meta data-vue-meta="ssr" property="og:title" content="Вижу, значит существую: обзор Deep Learning в Computer Vision (часть 1)" data-vmid="og:title"><meta data-vue-meta="ssr" name="twitter:title" content="Вижу, значит существую: обзор Deep Learning в Computer Vision (часть 1)" data-vmid="twitter:title"><meta data-vue-meta="ssr" name="aiturec:title" content="Вижу, значит существую: обзор Deep Learning в Computer Vision (часть 1)" data-vmid="aiturec:title"><meta data-vue-meta="ssr" name="description" content="Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами...." data-vmid="description"><meta data-vue-meta="ssr" itemprop="description" content="Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами...." data-vmid="description:itemprop"><meta data-vue-meta="ssr" property="og:description" content="Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами...." data-vmid="og:description"><meta data-vue-meta="ssr" name="twitter:description" content="Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами...." data-vmid="twitter:description"><meta data-vue-meta="ssr" property="aiturec:description" content="Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами...." data-vmid="aiturec:description"><meta data-vue-meta="ssr" itemprop="image" content="https://habr.com/share/publication/450732/77baa3f666e996ccfb66cadf1cc3ee90/" data-vmid="image:itemprop"><meta data-vue-meta="ssr" property="og:image" content="https://habr.com/share/publication/450732/77baa3f666e996ccfb66cadf1cc3ee90/" data-vmid="og:image"><meta data-vue-meta="ssr" property="aiturec:image" content="https://habr.com/share/publication/450732/77baa3f666e996ccfb66cadf1cc3ee90/" data-vmid="aiturec:image"><meta data-vue-meta="ssr" name="twitter:image" content="https://habr.com/share/publication/450732/77baa3f666e996ccfb66cadf1cc3ee90/" data-vmid="twitter:image"><meta data-vue-meta="ssr" property="vk:image" content="https://habr.com/share/publication/450732/77baa3f666e996ccfb66cadf1cc3ee90/" data-vmid="vk:image"><meta data-vue-meta="ssr" property="aiturec:item_id" content="450732" data-vmid="aiturec:item_id"><meta data-vue-meta="ssr" property="aiturec:datetime" content="2019-05-21T11:31:04.000Z" data-vmid="aiturec:datetime"><meta data-vue-meta="ssr" property="og:type" content="article" data-vmid="og:type"><meta data-vue-meta="ssr" property="og:locale" content="ru_RU" data-vmid="og:locale"><meta data-vue-meta="ssr" property="og:image:width" content="1200" data-vmid="og:image:width"><meta data-vue-meta="ssr" property="og:image:height" content="630" data-vmid="og:image:height">
  <link data-vue-meta="ssr" href="https://habr.com/ru/rss/post/450732/?fl=ru" type="application/rss+xml" title="" rel="alternate" name="rss"><link data-vue-meta="ssr" href="https://habr.com/ru/company/mipt/blog/450732/" rel="canonical" data-vmid="canonical"><link data-vue-meta="ssr" data-vmid="hreflang"><link data-vue-meta="ssr" image_src="image" href="https://habr.com/share/publication/450732/77baa3f666e996ccfb66cadf1cc3ee90/" data-vmid="image:href">
  <meta name="apple-mobile-web-app-status-bar-style" content="#303b44">
  <meta name="msapplication-TileColor" content="#629FBC">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="16x16"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-16.png"
  >
  <link
    rel="shortcut icon"
    type="image/png"
    sizes="32x32"
    href="https://assets.habr.com/habr-web/img/favicons/favicon-32.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="76x76"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-76.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="120x120"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="152x152"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-152.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="180x180"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-180.png"
  >
  <link
    rel="apple-touch-icon"
    type="image/png"
    sizes="256x256"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-256.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1136x640.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2436x1125.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1792x828.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_828x1792.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1334x750.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2208x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 812px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1125x2436.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 736px) and (-webkit-device-pixel-ratio: 3) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1242x2208.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2732x2048.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 414px) and (device-height: 896px) and (-webkit-device-pixel-ratio: 3) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2688x1242.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2224x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 375px) and (device-height: 667px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_750x1334.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 1024px) and (device-height: 1366px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x2732.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2388x1668.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1112px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2224.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 320px) and (device-height: 568px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_640x1136.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 834px) and (device-height: 1194px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1668x2388.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: landscape)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_2048x1536.png"
  >
  <link
    rel="apple-touch-startup-image"
    media="screen and (device-width: 768px) and (device-height: 1024px) and (-webkit-device-pixel-ratio: 2) and (orientation: portrait)"
    href="https://assets.habr.com/habr-web/img/splashes/splash_1536x2048.png"
  >
  <link
    rel="mask-icon"
    color="#77a2b6"
    href="https://assets.habr.com/habr-web/img/favicons/apple-touch-icon-120.svg"
  >
  <link
    crossorigin="use-credentials"
    href="/manifest.webmanifest"
    rel="manifest"
  >
</head>
<body>


<div id="app" data-server-rendered="true" data-async-called="true"><div class="tm-layout__wrapper"><!----> <div></div> <!----> <header class="tm-header"><div class="tm-page-width"><div class="tm-header__container"><!----> <span class="tm-header__logo-wrap"><a href="/ru/" class="tm-header__logo tm-header__logo_ru"><svg height="16" width="16" class="tm-svg-img tm-header__icon"><title>Хабр</title> <use xlink:href="/img/habr-logo-ru.svg#logo"></use></svg></a> <span class="tm-header__beta-sign" style="display:none;">β</span></span> <div class="tm-dropdown tm-header__projects"><div class="tm-dropdown__head"><button class="tm-header__dropdown-toggle"><svg height="16" width="16" class="tm-svg-img tm-header__icon tm-header__icon_dropdown"><title>Открыть список</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#arrow-down"></use></svg></button></div> <!----></div> <a href="/ru/sandbox/start/" class="tm-header__become-author-btn">
            Как стать автором
          </a> <div class="tm-feature tm-header__feature tm-feature_variant-inline"><!----></div> <!----> <!----></div></div></header> <div class="tm-layout"><div class="tm-page-progress-bar"></div> <div data-menu-sticky="true" class="tm-base-layout__header tm-base-layout__header_is-sticky"><div class="tm-page-width"><div class="tm-base-layout__header-wrapper"><div class="tm-main-menu"><div class="tm-main-menu__section"><nav class="tm-main-menu__section-content"><!----> <a href="/ru/all/" class="tm-main-menu__item">
        Все потоки
      </a> <a href="/ru/flows/develop/" class="tm-main-menu__item">
          Разработка
        </a><a href="/ru/flows/admin/" class="tm-main-menu__item">
          Администрирование
        </a><a href="/ru/flows/design/" class="tm-main-menu__item">
          Дизайн
        </a><a href="/ru/flows/management/" class="tm-main-menu__item">
          Менеджмент
        </a><a href="/ru/flows/marketing/" class="tm-main-menu__item">
          Маркетинг
        </a><a href="/ru/flows/popsci/" class="tm-main-menu__item">
          Научпоп
        </a></nav></div></div> <div class="tm-header-user-menu tm-base-layout__user-menu"><a href="/ru/search/" class="tm-header-user-menu__item tm-header-user-menu__search"><svg height="24" width="24" class="tm-svg-img tm-header-user-menu__icon tm-header-user-menu__icon_search tm-header-user-menu__icon_dark"><title>Поиск</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#search"></use></svg></a> <!----> <!----> <!----> <div class="tm-header-user-menu__item tm-header-user-menu__user_desktop"><div class="tm-dropdown"><div class="tm-dropdown__head"><svg height="24" width="24" data-test-id="menu-toggle-guest" class="tm-svg-img tm-header-user-menu__icon"><title>Профиль</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#header-user"></use></svg> <!----></div> <!----></div> <!----></div> <!----></div></div></div></div> <!----> <div class="tm-page-width"></div> <main class="tm-layout__container"><div hl="ru" companyName="mipt" data-async-called="true" class="tm-page"><div class="tm-page-width"><div class="tm-page__header"><!----></div> <div class="tm-page__wrapper"><div class="tm-page__main tm-page__main_has-sidebar"><div class="pull-down"><div class="pull-down__header" style="height:0px;"><div class="pull-down__content" style="bottom:10px;"><svg height="24" width="24" class="tm-svg-img pull-down__arrow"><title>Обновить</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#pull-arrow"></use></svg></div></div> <div class="tm-article-presenter"><div class="tm-company-card tm-company-article__company-card"><div class="tm-company-card__info"><div class="tm-company-card__header"><a href="/ru/company/mipt/profile/" class="tm-company-card__avatar"><div class="tm-entity-image"><img alt="" height="48" src="//habrastorage.org/getpro/habr/company/70d/544/daf/70d544daf4932b17afc9dd3efa0a9cf3.jpg" width="48" class="tm-entity-image__pic"></div></a> <!----> <div class="tm-rating tm-company-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">57.62</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div> <div class="tm-company-card__info"><a href="/ru/company/mipt/profile/" class="tm-company-card__name">
        Московский физико-технический институт (МФТИ)
      </a> <div class="tm-company-card__description"></div></div></div> <div class="tm-company-card__buttons"><!----> <!----></div></div> <div class="tm-article-presenter__body"><div class="tm-misprint-area"><div class="tm-misprint-area__wrapper"><article class="tm-article-presenter__content tm-article-presenter__content_narrow"><div class="tm-article-presenter__header"> <div class="tm-article-snippet tm-article-presenter__snippet"><div class="tm-article-snippet__meta-container"><div class="tm-article-snippet__meta"><span class="tm-user-info tm-article-snippet__author"><a href="/ru/users/izakharkin/" title="izakharkin" class="tm-user-info__userpic"><div class="tm-entity-image"><img alt="" height="24" loading="lazy" src="//habrastorage.org/r/w32/getpro/habr/avatars/aa8/0aa/da7/aa80aada722e56f3beec6ac3c651de4a.jpg" width="24" class="tm-entity-image__pic"></div></a> <span class="tm-user-info__user"><a href="/ru/users/izakharkin/" class="tm-user-info__username">
      izakharkin
    </a> </span></span> <span class="tm-article-snippet__datetime-published"><time datetime="2019-05-21T11:31:04.000Z" title="2019-05-21, 14:31">21  мая  2019 в 14:31</time></span></div> <!----></div> <h1 lang="ru" class="tm-article-snippet__title tm-article-snippet__title_h1"><span>Вижу, значит существую: обзор Deep Learning в Computer Vision (часть 1)</span></h1> <div class="tm-article-snippet__hubs"><span class="tm-article-snippet__hubs-item"><a href="/ru/company/mipt/blog/" class="tm-article-snippet__hubs-item-link router-link-active"><span>Блог компании Московский физико-технический институт (МФТИ)</span> <!----></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/programming/" class="tm-article-snippet__hubs-item-link"><span>Программирование</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/image_processing/" class="tm-article-snippet__hubs-item-link"><span>Обработка изображений</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/machine_learning/" class="tm-article-snippet__hubs-item-link"><span>Машинное обучение</span> <span title="Профильный хаб" class="tm-article-snippet__profiled-hub">*</span></a></span><span class="tm-article-snippet__hubs-item"><a href="/ru/hub/artificial_intelligence/" class="tm-article-snippet__hubs-item-link"><span>Искусственный интеллект</span> <!----></a></span></div> <!----> <!----> <!----></div></div> <!----> <div data-gallery-root="" lang="ru" class="tm-article-body"><div id="post-content-body" class="article-formatted-body article-formatted-body_version-1"><div xmlns="http://www.w3.org/1999/xhtml">Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами. А ведь их очень много, и они правда крутые! Если вам интересно, что сейчас происходит в области Computer Vision не только с точки зрения <a href="https://arxiv.org/list/cs.CV/Recent">исследований и статей</a>, но и с точки зрения прикладных задач, то милости прошу под кат. Также статья может стать неплохим введением для тех, кто давно хотел начать разбираться во всём этом, но что-то мешало ;)<br/>
<br/>
<img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" alt="image" data-src="https://habrastorage.org/getpro/habr/post_images/ecb/319/e06/ecb319e06d692a5ea4f2a1343cf9c31d.jpg" data-blurred="true"/><br/>
<a name="habracut"></a><br/>
Сегодня на Физтехе происходит активная коллаборация «академии» и индустриальных партнёров. В частности, в <a href="https://mipt.ru/education/departments/fpmi/">Физтех-школе Прикладной математики и информатики</a> действуют множество <a href="https://mipt.ru/education/departments/fpmi/labs/">интересных лабораторий</a> от таких компаний, как Сбербанк, Biocad, 1С, Тинькофф, МТС, Huawei. <br/>
<br/>
На написание этой статьи меня вдохновила работа в Лаборатории <a href="http://neuruslab.ru/">гибридных интеллектуальных систем</a>, открытой компанией <a href="https://new.vkusvill.ru/">ВкусВилл</a>. У лаборатории амбициозная задача — построить магазин, работающий без касс, в основном при помощи компьютерного зрения. За почти год работы мне довелось поработать над многими задачами зрения, о которых и пойдёт речь в этих двух частях.<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Магазин без касс? Где-то я это уже слышал..</b><div class="spoiler_text">Наверное, дорогой читатель, Вы подумали про <a href="https://habr.com/ru/post/399725/">Amazon Go</a>. В каком-то смысле стоит задача повторить их успех, однако наше решение больше про внедрение, нежели про построение такого магазина с нуля за <a href="https://www.bloomberg.com/news/articles/2018-09-20/amazon-could-spend-3-billion-on-go-stores-analyst-says">огромные деньги</a>.<br/>
</div></div><br/>
Будем двигаться по плану:<br/>
<br/>
<ol>
<li><a href="#1">Мотивация и что вообще происходит</a></li>
<li><a href="#2">Классификация как стиль жизни</a></li>
<li><a href="#3">Архитектуры свёрточных нейросетей: 1000 способов достичь одной цели </a></li>
<li><a href="#4">Визуализация свёрточных нейросетей: покажи мне страсть</a></li>
<li><a href="#5">Я и сам своего рода хирург: извлекаем фичи из нейросетей</a></li>
<li><a href="#6">Держись рядом: representation learning для людей и лиц</a></li>
<li><a href="#7">Часть 2: <s>детектирование, оценка позы и распознавание действий</s> без спойлеров</a></li>
</ol><br/>
<a name="1"></a><h2>Мотивация и что вообще происходит</h2><br/>
<div class="spoiler"><b class="spoiler_title">Для кого статья?</b><div class="spoiler_text">Статья ориентирована в большей степени на людей, которые уже знакомы с машинным обучением и нейросетями. Однако советую прочитать хотя бы первые два раздела — вдруг всё будет понятно :)<br/>
</div></div><br/>
В 2019 году все говорят про искусственный интеллект, <a href="https://meduza.io/cards/ekonomisty-obsuzhdayut-chetvertuyu-promyshlennuyu-revolyutsiyu-chto-eto">четвёртую промышленную революцию</a> и <a href="https://socialego.mediasole.ru/rey_kurcveyl_raspisal_buduschee_mira_prognoz_do_2099_goda">приближение человечества к сингулярности</a>. Круто, классно, но хочется конкретики. Ведь мы с вами любопытные технари, которые не верят в сказки про ИИ, мы верим в формальную постановку задач, математику и программирование. В этой статье мы поговорим о конкретных кейсах применения того самого современного ИИ — о применении deep learning (а именно — свёрточных нейросетей) в множестве задач компьютерного зрения. <br/>
<br/>
Да, мы будем говорить именно про сетки, иногда упоминая некоторые идеи из «классического» зрения (так будем называть набор методов в зрении, которые использовались до нейросетей, однако это ни в коем случае не значит, что сейчас они не используются). <br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Хочу изучить компьютерное зрение с нуля</b><div class="spoiler_text">Рекомендую <a href="https://www.lektorium.tv/course/22847">курс Антона Конушина «Введение в компьютерное зрение»</a>. Лично я проходил его аналог в ШАДе, что заложило прочную основу в понимании обработки изображений и видео.<br/>
</div></div><br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" alt="image" width="300" data-src="https://habrastorage.org/webt/gu/vu/o3/guvuo3vejwwjimlpcqiwgbpxldq.jpeg" data-blurred="true"/></div><br/>
На мой взгляд, первое действительно интересное применение нейросетей в зрении, которое было освещено в СМИ ещё в 1993 году, это <a href="https://www.youtube.com/watch?v=FwFduRA_L6Q&amp;feature=youtu.be">распознавание рукописных цифр</a>, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-90c.pdf">реализованное Яном ЛеКуном</a>. Сейчас он один из главных по ИИ в <a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research</a>, их команда выпустила уже <a href="https://opensource.facebook.com/#artificial-intelligence">немало полезных Open Source вещей</a>.<br/>
<br/>
Сегодня же зрение применяется во многих сферах. Приведу лишь несколько ярких примеров: <br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" alt="image" width="400" data-src="https://habrastorage.org/webt/3x/tl/-j/3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg" data-blurred="true"/></div><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" alt="image" width="400" height="300" data-src="https://habrastorage.org/getpro/habr/post_images/967/987/50c/96798750c04282d6514f994b8375edcb.jpg" data-blurred="true"/></div><br/>
<br/>
<i>Беспилотные автомобили <a href="https://www.tesla.com/autopilot">Tesla</a> и <a href="https://yandex.ru/promo/taxi/sdc">Яндекса</a></i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" alt="image" width="400" data-src="https://habrastorage.org/getpro/habr/post_images/dda/997/082/dda9970829bfb17bb2b118a08d519835.jpg" data-blurred="true"/></div><br/>
<br/>
<i><a href="https://academic.oup.com/jnci/advance-article-abstract/doi/10.1093/jnci/djy222/5307077?redirectedFrom=fulltext">Анализ медицинских снимков</a> и <a href="https://www.nature.com/articles/s41591-018-0177-5.epdf?referrer_access_token=Vcd6TxYnYcWaq1GBsBATUdRgN0jAjWel9jnR3ZoTv0NHburD9_XZ7WMEhIlD9p3NpWLpsg4XdGg7RALozn59eUhEzW5MtONSBjWG2OZXXgEmuDAUdQU8Uba-wwWSDO7SEqELRaDpZA4VgrIZd-yoJVelT9hlYSARgD6lX1MUs8Pfx2qBop3sw39xD2kYf-mO&amp;tracking_referrer=www.wired.com">предсказание рака</a></i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" alt="image" width="500" data-src="https://habrastorage.org/webt/jz/9k/2o/jz9k2ovcurxg4zd_cj_kb20hs_0.jpeg" data-blurred="true"/></div><br/>
<br/>
<i>Игровые приставки: Kinect 2.0 (правда, там ещё используется информация о глубине, то есть RGB-D картинки)</i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" alt="image" width="400" data-src="https://habrastorage.org/getpro/habr/post_images/4d1/fb8/125/4d1fb8125d4624b40993f441b42ac48d.jpg" data-blurred="true"/></div><br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" width="400" data-src="https://habrastorage.org/webt/wf/kw/la/wfkwlap8pltophsuh1ggkxgkii8.jpeg" data-blurred="true"/></div><br/>
<br/>
<i>Распознавание по лицу: <a href="https://support.apple.com/ru-ru/HT208109">Apple FaceID</a> (при помощи нескольких датчиков)</i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" alt="image" width="400" data-src="https://habrastorage.org/getpro/habr/post_images/2d3/f3b/178/2d3f3b17818ae279e7a47d3c940e002f.jpg" data-blurred="true"/></div><br/>
<br/>
<i>Оценка точек лица: <a href="https://www.youtube.com/watch?v=Pc2aJxnmzh0">маски в Snapchat</a></i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" alt="image" width="400" data-src="https://habrastorage.org/webt/hn/cw/oc/hncwocoggiei8lkijpl8ihgbx_o.jpeg" data-blurred="true"/></div><br/>
<br/>
<i>Биометрия лица и движений глаз (пример из <a href="https://mipt.ru/science/labs/mipt-sberbank-applied-research/projects/novaya_biometricheskaya_autentifikatsiya">проекта ФПМИ МФТИ</a>)</i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="/img/image-loader.svg" alt="image" data-src="https://habrastorage.org/webt/vg/vv/4f/vgvv4f_ddwswudk1yvghxjl4rne.png" data-width="400"/></div><br/>
<br/>
<i>Поиск по картинке: Яндекс и Google</i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" alt="image" width="500" data-src="https://habrastorage.org/getpro/habr/post_images/b4d/cfd/d13/b4dcfdd13f85affc79d876cf4bd3f4fd.jpg" data-blurred="true"/></div><br/>
<br/>
<i>Распознавание текста на картинке (<a href="https://ru.wikipedia.org/wiki/%D0%9E%D0%BF%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D0%BE%D0%B2">Optical Character Recognition</a>)</i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" alt="image" width="400" data-src="https://habrastorage.org/getpro/habr/post_images/cfa/2bb/afa/cfa2bbafae96a5bd082ef25bae9d19af.jpg" data-blurred="true"/></div><br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" alt="image" width="400" data-src="https://habrastorage.org/getpro/habr/post_images/60d/62d/670/60d62d670999dcc7cbd726dde47905a0.jpg" data-blurred="true"/></div><br/>
<br/>
<i>Дроны и роботы: получение и обработка информации с помощью зрения</i><br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" alt="image" width="500" data-src="https://habrastorage.org/getpro/habr/post_images/113/220/ca0/113220ca03176c5a99b82819076e0c8a.jpg" data-blurred="true"/></div><br/>
<i><a href="https://habr.com/ru/post/404757/">Одометрия</a>: построение карты и планирование при перемещении роботов</i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="/img/image-loader.svg" alt="image" data-src="https://habrastorage.org/webt/ju/b7/i6/jub7i61z3oiairdg2q45x0l6loi.png" data-width="500"/></div><br/>
<br/>
<i><a href="https://evrl.to/articles/5c1f4c056ec7f73533eac24b/kak-nejronnye-seti-uluchshajut-grafiku-v-staryh-igrah/">Улучшение графики и текстур в видеоиграх</a></i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" alt="image" width="200" height="300" data-src="https://habrastorage.org/getpro/habr/post_images/d56/155/0ee/d561550eec9f5badc4475392a584fe03.jpg" data-blurred="true"/></div><br/>
<br/>
<i>Перевод по картинке: Яндекс и Google</i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" alt="image" width="500" data-src="https://habrastorage.org/getpro/habr/post_images/859/ffd/2d5/859ffd2d56f231c5f9b802978a688c94.jpg" data-blurred="true"/></div><br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" alt="image" width="500" data-src="https://habrastorage.org/getpro/habr/post_images/31f/003/a47/31f003a47c5dc5b5c5f75758d4d3689c.jpg" data-blurred="true"/></div><br/>
<br/>
<i>Дополненная реальность: <a href="https://developer.leapmotion.com/northstar">Leap Motion (Project North Star)</a> и <a href="https://www.microsoft.com/en-ca/hololens">Microsoft Hololens</a></i><br/>
<br/>
<p></p><div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" alt="image" width="250" height="200" data-src="https://habrastorage.org/getpro/habr/post_images/9a4/3ea/74b/9a43ea74ba0b5595f257feb313756293.jpg" data-blurred="true"/></div><br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" width="250" height="300" data-src="https://habrastorage.org/getpro/habr/post_images/e90/962/25b/e9096225bb7d5799823737c960e19ad6.jpg" data-blurred="true"/></div><br/>
<br/>
<i>Перенос стиля и текстур: <a href="https://prisma-ai.com/">Prisma</a>, <a href="https://picsart.com/">PicsArt</a></i><br/>
<br/>
Я уже не говорю про многочисленные применения в различных внутренних задачах компаний. Facebook, к примеру, применяет зрение ещё и для того, чтобы фильтровать медиаконтент. В <a href="https://blogs.nvidia.com/blog/2019/04/18/lucidyne-gradescan-lumber-grading/">проверке качества/повреждений в промышленности</a> тоже используются методы компьютерного зрения.<br/>
<br/>
Дополненной реальности здесь нужно, на самом деле, уделить отдельное внимание, поскольку <s>она не работает</s> в скором времени это может стать одной из главных областей применения зрения.<br/>
<br/>
Смотивировались. Зарядились. Поехали:<br/>
<br/>
<a name="2"></a><h2>Классификация как стиль жизни</h2><br/>
<br/>
<img src="/img/image-loader.svg" alt="image" data-src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/38211dc39e41273c0007889202c69f841e02248a/2-Figure1-1.png"/><br/>
<br/>
Как я уже говорил, в 90-е годы сетки в зрении выстрелили. Причём выстрелили в конкретной задаче — задаче классификации картинок рукописных цифр (знаменитый <a href="https://www.kaggle.com/c/digit-recognizer/data">датасет MNIST</a>). Исторически сложилось, что именно задача классификации изображений и стала основой при решении почти всех последующих задач в зрении. Рассмотрим конкретный пример:<br/>
<br/>
<b>Задача</b>: На вход дана папка с фотографиями, на каждом фото тот или иной объект: либо кошка, либо собака, либо человек (пусть “мусорных” фоток нет, супер-не-жизненная задача, но надо с чего-то начать). Нужно разложить картинки по трём папкам: <code>/cats</code>, <code>/dogs</code> и <s><code>/leather_bags</code></s> <code>/humans</code>, поместив в каждую папку только фото с соответствующими объектами.<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Что такое картинка/фотография?</b><div class="spoiler_text"><img src="/img/image-loader.svg" alt="image" data-src="https://habrastorage.org/getpro/habr/post_images/074/e15/f04/074e15f04c8347ab32f98ba04aeceb6c.png"/><br/>
Практически везде в зрении принято работать с картинками в RGB-формате. У каждой картинки есть высота (H), ширина (W) и глубина, которая равна 3 (цвета). Таким образом, одну картинку можно представить как тензор размерности HxWx3 (каждый пиксель — это набор из трёх чисел — значений интенсивности в каналах). <br/>
</div></div><br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" width="400" data-src="https://habrastorage.org/getpro/habr/post_images/26c/167/e3f/26c167e3feb823e778b32278358053f9.jpg" data-blurred="true"/></div><br/>
<br/>
Представим, что с компьютерным зрением мы пока не знакомы, но знаем machine learning. Изображения — это просто числовые тензоры в памяти компьютера. Формализуем задачу в терминах машинного обучения: объекты — это картинки, их признаки — это значения в пикселях, ответ для каждого из объектов — метка класса (кошка, собака или человек). Это в чистом виде <a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F">задача классификации</a>. <br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Если сейчас уже стало сложно..</b><div class="spoiler_text">… то лучше сначала прочитать первые 4 статьи из<a href="https://habr.com/ru/company/ods/blog/322534/"> Открытого курса OpenDataScience по ML</a> и ознакомиться с более вводной статьёй по зрению, например, <a href="https://habr.com/ru/company/yandex/blog/203136/">хорошая лекция в Малом ШАДе</a>. <br/>
</div></div><br/>
Можно взять какие-нибудь методы из “классического” зрения или “классического” машинного обучения, то есть не нейросети. В основном эти методы заключаются в выделении на изображениях неких особенностей (особых точек) или локальных регионов, которые будут характеризовать картинку (“<a href="https://towardsdatascience.com/bag-of-visual-words-in-a-nutshell-9ceea97ce0fb">мешок визуальных слов</a>”). Обычно всё это сводится к чему-то типа <a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2">SVM </a>над <a href="https://ru.wikipedia.org/wiki/%D0%93%D0%B8%D1%81%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D0%BD%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D1%8B%D1%85_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%B2">HOG</a>/<a href="https://habr.com/ru/post/106302/">SIFT</a>. <br/>
<br/>
Но мы здесь собрались, чтобы поговорить о нейросетях, поэтому не хотим использовать придуманные нами признаки, а хотим, чтобы сеть сделала всё за нас. Наш классификатор будет принимать на вход признаки объекта и возвращать предсказание (метку класса). Здесь в качестве признаков выступают значения интенсивности в пикселях (см. модель картинки в под <br/>
спойлером выше). Помним, что картинка — это тензор размера (Height, Width, 3) (если она цветная). Сетке при обучении на вход всё это обычно подаётся не по одной картинке и не целым датасетом, а батчами, т.е. небольшими порциями объектов (например, 64 картинки в батче).<br/>
<br/>
Таким образом, сеть принимает на вход тензор размера (BATCH_SIZE, H, W, 3). Можно “развернуть” каждую картинку в вектор-строку из H*W*3 чисел и работать со значениями в пикселях прямо как с признаками в машинном обучении, обычный <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer Perceptron (MLP)</a> так и поступил бы, но это, честно говоря, такой себе бейзлайн, поскольку работа с пикселями как с вектор-строкой никак не учитывает, например, трансляционную инвариантность объектов на картинке. Тот же кот может быть как в середине фото, так и в углу, MLP эту закономерность не выучит.<br/>
<br/>
Значит нужно что-то поумнее, например, операция свёртки. И это уже про современное зрение, про <b><a href="https://habr.com/ru/post/348000/">свёрточные нейронные сети</a></b>:<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Код обучения свёрточной сети может выглядеть как-то так (на фреймворке PyTorch)</b><div class="spoiler_text"><pre><code class="python"># взято из официального туториала:
# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

for epoch in range(2):  # loop over the dataset multiple times
    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
</code></pre><br/>
</div></div><br/>
Поскольку сейчас речь об <a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81_%D1%83%D1%87%D0%B8%D1%82%D0%B5%D0%BB%D0%B5%D0%BC">обучении с учителем</a>, для тренировки нейросети нам нужны несколько компонент:<br/>
<br/>
<ul>
<li>Данные (уже есть)</li>
<li>Архитектура сети (самое интересное)</li>
<li>Функция потерь, которая будет говорить, как нейросети учиться (здесь это будет <a href="https://medium.com/@vijendra1125/understanding-entropy-cross-entropy-and-softmax-3b79d9b23c8a">кросс-энтропия</a>)</li>
<li>Метод оптимизации (будет менять веса сети в нужную сторону)</li>
<li>Задать гиперпараметры архитектуры и оптимизатора (например, размер шага оптимизатора, количество нейронов в слоях, коэффициенты регуляризации)</li>
</ul><br/>
В коде именно это и реализовано, сама свёрточная нейросеть описана в классе Net().<br/>
<br/>
Если хочется не спеша и с начала узнать про свёртки и свёрточные сети, рекомендую <a href="https://www.youtube.com/watch?v=Xul1DS08hSA&amp;list=PL0Ks75aof3ThkitsZbUOEQg7Ybl5kB_s3&amp;index=16">лекцию в Deep Learning School (ФПМИ МФТИ) (на русском)</a> на эту тему, и, конечно же, <a href="https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv">курс Стэнфорда cs231n (на английском)</a>.<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Deep Learning School -- что это?</b><div class="spoiler_text"><a href="https://www.dlschool.org/">Deep Learning School</a> при <a href="https://mipt.ru/science/labs/innovation/">Лаборатории Инноватики ФПМИ МФТИ</a> — это организация, которая активно занимается разработкой открытого русскоязычного курса по нейросетям. В статье я буду несколько раз ссылаться на эти <a href="https://www.youtube.com/watch?v=RviskFqwF3M&amp;list=PL0Ks75aof3ThkitsZbUOEQg7Ybl5kB_s3">видеоуроки</a>.<br/>
</div></div><br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" alt="image" width="650" data-src="https://habrastorage.org/getpro/habr/post_images/fb1/3ca/d97/fb13cad97db640053bb2c53c12b0f4a7.jpg" data-blurred="true"/></div><br/>
Если вкратце, то операция свёртки позволяет находить паттерны на изображениях с учётом их вариативности. Когда обучаем свёрточные нейросети (eng: Convolutional Neural Networks), мы, по сути, находим фильтры свёрток (веса нейронов), которые хорошо описывают изображения, причём столь хорошо, чтобы можно было точно определить по ним класс. Способов построить такую сеть придумали много. Больше, чем вы думаете…<br/>
<br/>
<a name="3"></a><h3> Архитектуры свёрточных нейросетей: 1000 способов достичь одной цели</h3><br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" alt="image" data-src="https://habrastorage.org/getpro/habr/post_images/c28/ab9/3c6/c28ab93c670c1e44258dc86064bb3a0c.png" data-width="500"/></div><br/>
<br/>
Да-да, <a href="https://habr.com/ru/company/nixsolutions/blog/430524/">ещё один обзор архитектур</a>. Но здесь я постараюсь сделать его максимально актуальным!<br/>
<br/>
Сначала была <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">LeNet</a>, она помогла Яну ЛеКуну распознавать цифры в 1998 году. Это была первая свёрточная нейросеть для классификации. Её основная фишка была в том, что она в принципе стала использовать <a href="https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050">convolution и pooling</a> операции.<br/>
<br/>
<img src="/img/image-loader.svg" alt="image" data-src="https://habrastorage.org/getpro/habr/post_images/bd4/27e/5e2/bd427e5e2943ebf58409e42538c4e131.png"/><br/>
<br/>
Далее было некоторое затишье в развитии сеток, однако «железо» не стояло на месте, развивались эффективные вычисления на GPU и <abbr title="Accelerated Linear Algebra">XLA</abbr>. В 2012 году появилась AlexNet, она выстрелила в соревновании ILSVRC (<a href="http://www.image-net.org/challenges/LSVRC/">ImageNet Large-Scale Visual Recognition Challenge</a>). <br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Небольшое отступление про ILSVRC</b><div class="spoiler_text">К 2012 году был дан собран <a href="http://www.image-net.org">ImageNet</a>, и для соревнования ILSVRC использовалась его подвыборка из тысяч картинок и 1000 классов. Сейчас в ImageNet ~14 миллионов картинок и 21841 класс (взято с оф. сайта), но для соревнования всё равно обычно выделяют лишь подмножество. ILSVRC тогда стал самым крупным ежегодным соревнованием по классификации изображений. Кстати, недавно придумали, как можно <a href="https://arxiv.org/pdf/1709.05011.pdf">обучить на ImageNet'е за считанные минуты</a>. <br/>
<br/>
Именно на ImageNet (в ILSVRC) с 2010 по 2018 получали <abbr title="State-of-the-Art">SOTA</abbr>-нейросети в задаче классификации изображений. Правда уже с 2016 года более актуальны соревнования по локализации, детектирования и понимания сцены, а не классификацию.<br/>
</div></div><br/>
Обычно различные <a href="https://medium.com/@RaghavPrabhu/cnn-architectures-lenet-alexnet-vgg-googlenet-and-resnet-7c81c017b848">обзоры архитектур</a> проливают свет на те, что были первыми на ILSVRC с 2010 до 2016 года, и на некоторые отдельные сети. Чтобы не загромождать рассказ, я поместил их под спойлер ниже, постаравшись подчеркнуть основные идеи:<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Архитектуры с 2012 по 2015 год</b><div class="spoiler_text"><div class="scrollable-table"><table>
<tr>
<th>Год</th>
<th>Статья</th>
<th>Ключевая идея</th>
<th>Вес</th>
</tr>
<tr>
<td>2012</td>
<td><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a></td>
<td>использовать две свёртки подряд; делить обучение сети на две параллельных ветки</td>
<td>240 MB</td>
</tr>
<tr>
<td>2013</td>
<td><a href="https://arxiv.org/pdf/1311.2901v3.pdf">ZFNet</a></td>
<td>размер фильтров, число фильтров в слоях</td>
<td>--</td>
</tr>
<tr>
<td>2013</td>
<td><a href="https://arxiv.org/abs/1312.6229">Overfeat</a></td>
<td>один из первых нейросетевых детекторов</td>
<td>--</td>
</tr>
<tr>
<td>2014</td>
<td><a href="https://arxiv.org/abs/1409.1556">VGG</a></td>
<td>глубина сети (13-19 слоёв), использование нескольких блоков Conv-Conv-Pool с меньшим размером свёрток (3х3)</td>
<td>549MB (VGG-19)</td>
</tr>
<tr>
<td>2014</td>
<td><a href="https://arxiv.org/abs/1409.4842">Inception (v1) (она же GoogLeNet)</a></td>
<td>1х1-свёртка (идея из <a href="https://arxiv.org/pdf/1312.4400.pdf">Network-in-Network</a>), auxilary losses (или <a href="https://arxiv.org/abs/1505.02496">deep supervision</a>), стекинг выходов нескольких свёрток (Inception-блок)</td>
<td>--</td>
</tr>
<tr>
<td>2015</td>
<td><a href="https://arxiv.org/abs/1512.03385">ResNet</a></td>
<td><a href="https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035">residual connections</a>, очень большая глубина (152 слоя..)</td>
<td>98 MB (ResNet-50), 232 MB (ResNet-152)</td>
</tr>
</table></div><br/>
</div></div><br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/z3/i4/b4/z3i4b4pxfnulxzfszysn_usqn_c.png" data-width="500"/></div><br/>
<br/>
Идеи всех этих архитектур (кроме ZFNet, её обычно мало упоминают) в своё время были новым словом в нейросетях для зрения. Однако и после 2015 было ещё очень много важных улучшений, например, Inception-ResNet, Xception, DenseNet, SENet. Ниже я постарался собрать их в одном месте.<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Архитектуры с 2015 по 2019 год</b><div class="spoiler_text"><div class="scrollable-table"><table>
<tr>
<th>Год</th>
<th>Статья</th>
<th>Ключевая идея</th>
<th>Вес</th>
</tr>
<tr>
<td>2015</td>
<td><a href="https://arxiv.org/abs/1512.00567">Inception v2 и v3</a></td>
<td><a href="https://habr.com/ru/post/302242/">разложение свёрток в свёртки 1хN и Nx1</a></td>
<td>92 MB</td>
</tr>
<tr>
<td>2016</td>
<td><a href="https://arxiv.org/abs/1602.07261">Inception v4 и Inception-ResNet</a></td>
<td><a href="https://habr.com/ru/post/303196/">совмещение Inception и ResNet</a></td>
<td>215 MB</td>
</tr>
<tr>
<td>2016-17</td>
<td><a href="https://arxiv.org/abs/1611.05431">ResNeXt</a></td>
<td>2 место ILSVRC, использование многих веток ( “обобщённый” Inception-блок)</td>
<td>--</td>
</tr>
<tr>
<td>2017</td>
<td><a href="https://arxiv.org/abs/1610.02357">Xception</a></td>
<td><a href="https://habr.com/ru/post/347564/">depthwise separable convolution</a>, меньше весит при сравнимой с Inception точности</td>
<td>88 MB</td>
</tr>
<tr>
<td>2017</td>
<td><a href="https://arxiv.org/abs/1608.06993">DenseNet</a></td>
<td><a href="https://towardsdatascience.com/densenet-2810936aeebb">Dense-блок</a>; лёгкая, но точная</td>
<td>33 MB (DenseNet-121), 80 MB (DenseNet-201)</td>
</tr>
<tr>
<td>2018</td>
<td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SENet</a></td>
<td><a href="https://shaoanlu.wordpress.com/2017/08/17/senet-winner-of-imagenet-2017/">Squeeze-and-Excitation блок</a></td>
<td><a href="https://github.com/hujie-frank/SENet">46 MB (SENet-Inception), 440 MB (SENet-154)</a></td>
</tr>
</table></div><br/>
</div></div><br/>
Большинство из этих моделей для PyTorch можно найти <a href="https://github.com/Cadene/pretrained-models.pytorch">здесь</a>, а ещё есть вот <a href="https://github.com/aaron-xichen/pytorch-playground">такая классная штука</a>.<br/>
<br/>
Вы могли заметить, что всё это дело весит довольно много (хотелось бы 20 MB максимум, а то поменьше), в то время как сейчас повсеместно используют мобильные устройства и приобретает популярность <a href="https://habr.com/ru/post/259243/">IoT</a>, а значит сетки хочется использовать и там.<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Связь веса модели и скорости работы</b><div class="spoiler_text">Поскольку нейросети внутри себя лишь перемножают тензоры, то количество операций умножения (читай: количество весов) напрямую влияет на скорость работы (если не используются трудоёмкие пост- или предобработка). Сама скорость работы сети зависит от реализации (фреймворка), железа, на котором выполняется, и от размера входной картинки.<br/>
</div></div><br/>
Авторы многих статей пошли по пути изобретения быстрых архитектур, я собрал их методы под спойлером ниже:<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Легковесные архитектуры CNN</b><div class="spoiler_text"><div class="scrollable-table"><table>
<tr>
<th>Год</th>
<th>Статья</th>
<th>Ключевая идея</th>
<th>Вес</th>
<th>Пример реализации</th>
</tr>
<tr>
<td>2016</td>
<td><a href="https://arxiv.org/abs/1602.07360">SqueezeNet</a></td>
<td><a href="http://everything.explained.today/SqueezeNet/">FireModule сжатия-разжатия</a></td>
<td>0.5 MB</td>
<td><a href="https://github.com/DeepScale/SqueezeNet">Caffe</a></td>
</tr>
<tr>
<td>2017</td>
<td><a href="https://arxiv.org/abs/1707.07012">NASNet</a></td>
<td><a href="http://everything.explained.today/SqueezeNet/"></a><a href="https://habr.com/ru/post/408641/">получена нейронным поиском архитектур, это сеть из разряда AutoML</a></td>
<td>23 MB</td>
<td><a href="https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/nasnet.py">PyTorch</a></td>
</tr>
<tr>
<td>2017</td>
<td><a href="https://arxiv.org/abs/1707.01083">ShuffleNet</a></td>
<td><a href="https://medium.com/syncedreview/shufflenet-an-extremely-efficient-convolutional-neural-network-for-mobile-devices-72c6f5b01651">pointwise group conv, channel shuffle</a></td>
<td>--</td>
<td><a href="https://github.com/farmingyard/ShuffleNet">Caffe</a></td>
</tr>
<tr>
<td>2017</td>
<td><a href="https://arxiv.org/abs/1704.04861">MobileNet (v1)</a></td>
<td><a href="https://habr.com/ru/post/352804/">depthwise separable convolutions и много других трюков</a></td>
<td>16 MB</td>
<td><a href="https://github.com/Zehaos/MobileNet">TensorFlow</a></td>
</tr>
<tr>
<td>2018</td>
<td><a href="https://arxiv.org/abs/1801.04381">MobileNet (v2)</a></td>
<td><a href="https://habr.com/ru/post/352804/">рекомендую эту статью на Хабре</a></td>
<td>14 MB</td>
<td><a href="https://github.com/shicai/MobileNet-Caffe">Caffe</a></td>
</tr>
<tr>
<td>2018</td>
<td><a href="https://arxiv.org/abs/1803.10615">SqueezeNext</a></td>
<td>см. картиночки в оригинальном репозитории</td>
<td>--</td>
<td><a href="https://github.com/amirgholami/SqueezeNext">Caffe</a></td>
</tr>
<tr>
<td>2018</td>
<td><a href="https://arxiv.org/abs/1803.10615">MnasNet</a></td>
<td><a href="https://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html">нейропоиск архитектуры специально под мобильные устройства с помощью RL</a></td>
<td>~2 MB</td>
<td><a href="https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet">TensorFlow</a></td>
</tr>
<tr>
<td>2019</td>
<td><a href="https://arxiv.org/abs/1905.02244">MobileNet (v3)</a></td>
<td>она вышла, пока я писал статью :)</td>
<td>--</td>
<td><a href="https://github.com/Randl/MobileNetV3-pytorch">PyTorch</a></td>
</tr>
</table></div><br/>
</div></div><br/>
Цифры во всех таблицах <s>взяты с потолка</s> из репозиториев, из <a href="https://keras.io/applications/">таблицы Keras Applications</a> и из <a href="https://arxiv.org/abs/1605.07678">этой статьи</a>.<br/>
<br/>
Вы спросите: “Для чего ты написал про весь этот “зоопарк” моделей? И почему всё же задача классификации? Мы же хотим научить машины видеть, а классификация — лишь какая-то узкая задача..”. Дело в том, что нейросети для детектирования объектов, оценки позы/точек, ре-идентификации и поиска по картинке используют в качестве <b><abbr title="основа, дословно -- позвоночник">backbone</abbr></b> именно модели для классификации, и уже от них зависит 80% успеха. <br/>
<br/>
Но хочется как-то больше доверять CNN, а то напридумывали чёрных коробок, а что «внутри» — не очевидно. Чтобы лучше понимать механизм функционирования свёрточных сетей, исследователи придумали использовать визуализацию.<br/>
<br/>
<a name="4"></a><h3>Визуализация свёрточных нейросетей: покажи мне страсть</h3><br/>
Важным шагом к осознанию того, что происходит внутри свёрточных сетей, стала статья <a href="https://arxiv.org/abs/1311.2901">«Visualizing and Understanding Convolutional Networks»</a>. В ней авторы предложили несколько способов визуализации того, на что именно (на какие части картинки) реагируют нейроны в разных слоях CNN (рекомендую также посмотреть <a href="https://www.youtube.com/watch?v=6wcs6szJWMY&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=12">лекцию Стэнфорда на эту тему</a>). Результаты получились весьма впечатляющие: авторы показали, что первые слои свёрточной сети реагируют на какие-то «низкоуровневые вещи» по типу краёв/углов/линий, а последние слои реагируют уже на целые части изображений (см. картинку ниже), то есть уже несут в себе некоторую семантику.<br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" alt="image" data-src="https://habrastorage.org/webt/fz/vm/ym/fzvmymab57wgircssyfgxiaomvy.jpeg" data-blurred="true"/></div><br/>
<br/>
Далее <a href="http://yosinski.com/deepvis">проект по глубокой визуализации от Cornell University и компании</a> продвинул визуализацию ещё дальше, в то время как <a href="https://habr.com/ru/post/367643/">знаменитый DeepDream</a> научился искажать в <s>наркоманском</s> интересном стиле (ниже картинка с <a href="https://deepdreamgenerator.com/#gallery">deepdreamgenerator.com</a>). <br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" alt="image" width="500" data-src="https://habrastorage.org/getpro/habr/post_images/e55/809/63f/e5580963fdfb998bfe2103f4cbf5aa8c.jpg" data-blurred="true"/></div><br/>
<br/>
В 2017 году вышла <a href="https://distill.pub/2017/feature-visualization/">очень хорошая статья на Distill</a>, в которой они провели подробный анализ того, что «видит» каждый из слоёв, и совсем недавно (в марте 2019) Google изобрела <a href="https://distill.pub/2019/activation-atlas/">активационные атласы</a>: своеобразные карты, которые можно строить для каждого слоя сети, что приближает к понимаю общей картины работы CNN.<br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/b-/-k/iw/b--kiw7-vibdk8vpuzfxhbagkuu.png" data-width="700"/></div><br/>
<br/>
Если хочется самому поиграться с визуализацией, я бы рекомендовал <a href="https://github.com/tensorflow/lucid">Lucid </a> и <a href="https://tensorspace.org/html/playground/mobilenetv1.html">TensorSpace</a>.<br/>
<br/>
Окей, кажется, CNN и правда в некоторой степени можно верить. Нужно научиться использовать это и в других задачах, а не только в классификации. В этом нам помогут извлечение Embedding'ов картинок и Transfer Learning.<br/>
<br/>
<a name="5"></a><h2>Я и сам своего рода хирург: извлекаем фичи из нейросетей</h2><br/>
Представим, что есть картинка, и мы хотим найти похожие на неё визуально (так умеет, например, поиск по картинке в Яндекс.Картинки). Раньше (до нейросетей) инженеры для этого извлекали фичи вручную, например, придумывая что-то, что хорошо описывает картинку и позволит её сравнивать с другими. В основном, эти методы (<a href="https://ru.wikipedia.org/wiki/%D0%93%D0%B8%D1%81%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D0%BD%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D1%8B%D1%85_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%B2">HOG</a>, <a href="https://habr.com/ru/post/106302/">SIFT</a>) оперируют <a href="https://en.wikipedia.org/wiki/Image_gradient">градиентами картинок</a>, обычно именно эти штуки и называют «классическими» дескрипторами изображений. Особо интересующихся отсылаю к <a href="https://medium.com/machine-learning-world/feature-extraction-and-similar-image-search-with-opencv-for-newbies-3c59796bf774">статье</a> и к <a href="https://www.lektorium.tv/course/22847">курсу Антона Конушина</a> (это не реклама, просто курс хороший :)<br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" alt="image" width="500" data-src="https://habrastorage.org/getpro/habr/post_images/5cd/b10/ea8/5cdb10ea8f19fe29432265e906640a90.jpg" data-blurred="true"/></div><br/>
<br/>
Используя нейросети, мы можем не придумывать самому эти фичи и эвристики, а правильно обучить модель и потом <b>взять за признаки картинки выход одного или нескольких слоёв сети</b>.<br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" alt="image" data-src="https://habrastorage.org/getpro/habr/post_images/887/d76/eb4/887d76eb431bcaf434ff70e2e0f2d4b0.png" data-width="650"/></div><br/>
Посмотрев на все архитектуры выше поближе, становится понятно, что в CNN для классификации есть два этапа:<br/>
1). <b>Feature extractor</b> слои для выделения информативных фич из картинок с помощью свёрточных слоёв<br/>
2). Обучение поверх этих фич <b>Fully Connected (FC)</b> слоёв-классификаторов<br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" alt="image" data-src="https://habrastorage.org/getpro/habr/post_images/55d/ca5/358/55dca535836121c65546bc11e2d457c1.png" data-width="500"/></div><br/>
<br/>
<b>Embedding'и картинок (фичи)</b> — это как раз про то, что можно брать в качестве информативного описания картинок их признаки после Feature extractor’а свёрточной нейросети (правда их можно по-разному агрегировать). То есть обучили сеть на классификацию, а потом просто берём выход перед классификационными слоями. Эти признаки называют <i>фичами</i>, <i>нейросетевыми дескрипторами</i>или <i>эмбеддингами</i> картинки (правда обычно эмбеддинги принято в NLP, так как это зрение, я чаще буду говорить <i>фичи</i>). Обычно это какой-то числовой вектор, например, 128 чисел, с которым уже можно работать. <br/>
<br/>
<div class="spoiler"><b class="spoiler_title">А как же автоэнкодеры?</b><div class="spoiler_text">Да, на самом деле фичи можно получить и <a href="https://habr.com/ru/post/331382/">автоэнкодерами</a>. На моей практике делали по-разному, но, например, в статьях по ре-идентификации (о которой речь будет дальше), чаще всё же берут фичи после extractor'a, а не обучают для этого автоэнкодер. Мне кажется, стоит провести эксперименты в обоих направлениях, если стоит вопрос о том, что работает лучше.<br/>
</div></div><br/>
Таким образом, пайплайн решения <b>задачи поиска по картинке</b> может быть устроен просто: прогоняем картинки через CNN, берём признаки с нужных слоёв и сравниваем эти фичи друг с другом у разных картинок. Например, банально считаем Евклидово расстояние этих векторов.<br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" alt="image" data-src="http://api.ning.com/files/1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg/transferlearning.png" data-width="500"/></div><br/>
<br/>
<b>Transfer Learning</b> — широко известная техника эффективного дообучения нейросетей, которые уже обучены на каком-то определённом датасете, под свою задачу. Часто ещё говорят Fine Tuning вместо Transfer Learning, в <a href="http://cs231n.github.io/transfer-learning/">конспектах Стэнфордского курса cs231n</a> эти понятия разделяют, мол, Transfer Learning — это общая идея, а Fine Tuning — одна из реализаций техники. Нам это в дальнейшем не так важно, главное понимать, что мы просто можем дообучить сеть хорошо предсказывать на новом датасете, стартуя не со случайных весов, а с обученных на каком-нибудь большом по типу ImageNet. Это особенно актуально, когда данных мало, а задачу хочется решить качественно. <br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Подробнее про Transfer Learning</b><div class="spoiler_text"><a href="https://arxiv.org/abs/1411.1792">Оригинальная статья</a>, а вообще <a href="https://www.youtube.com/watch?v=U12tq9l9xy8">зачем читать много текста, если можно посмотреть видео</a><br/>
</div></div><br/>
Однако просто брать нужные фичи и делать дообучение с датасета на датасет может быть недостаточно, например, для задач поиска похожих лиц/людей/чего-то специфичного. Фотографии одного и того же человека визуально иногда могут быть даже более непохожи, чем фотографии разных людей. Нужно заставить сеть выделять именно те признаки, которые присущи одному человеку/объекту, даже если нам это сделать глазами сложно. Добро пожаловать в мир <b>representation learning</b>.<br/>
<br/>
<a name="6"></a><h2>Держись рядом: representation learning для людей и лиц</h2><br/>
<div class="spoiler"><b class="spoiler_title">Примечание по терминологии</b><div class="spoiler_text">Если почитать научные статьи, то иногда складывается впечатление, что некоторые авторы понимают словосочетание <b>metric learning</b> по-разному, и нет какого-то единого мнения на счёт того, какие методы называть metric learning, а какие нет. Именно поэтому в данной статье я решил избежать именно этого словосочетания и использовал более логичное <b>representation learning</b>, некоторые читатели могут с этим не согласиться — буду рад обсудить в комментариях.<br/>
</div></div><br/>
Поставим задачи:<br/>
<br/>
<ul>
<li><b>Задача 1</b>: есть галерея (набор) фотографий лиц людей, хотим, чтобы по новому фото сеть умела отвечать либо именем человека из галереи (мол, это он), либо говорила, что такого человека в галерее нет (и, возможно, добавляем в неё нового человека)<br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" width="300" data-src="https://habrastorage.org/getpro/habr/post_images/fbc/3ad/f28/fbc3adf280e28f7bb71246f50c1e8d9e.jpg" data-blurred="true"/></div></li>
<li><b>Задача 2</b>: то же самое, но работаем не с фотографиями лиц, а с кропами людей в полный рост<br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" width="400" data-src="https://habrastorage.org/webt/jh/43/xs/jh43xsjjgxixbw8cmo1idxage5a.jpeg" data-blurred="true"/></div></li>
</ul><br/>
<br/>
Первую задачу обычно называют <b>распознаванием лиц</b>, вторую — <b>ре-идентификацией</b> (сокращённо <i>Reid</i>). Я объединил их в один блок, поскольку в их решениях сегодня используются схожие идеи: для того, чтобы выучивать эффективные эмбеддинги картинок, которые могут справляться и с довольно сложными ситуациями, сегодня используют различные типы лоссов, такие как, например, <a href="https://arxiv.org/abs/1503.03832">triplet loss</a>, <a href="https://arxiv.org/pdf/1704.01719.pdf">quadruplet loss</a>, <a href="https://arxiv.org/pdf/1707.07391.pdf">contrastive-center loss</a>, <a href="https://elib.dlr.de/116408/1/WACV2018.pdf">cosine loss</a>. <br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" width="550" data-src="https://habrastorage.org/webt/9s/pj/cm/9spjcm6xbc2j2ip_wgri9wutjpi.jpeg" data-blurred="true"/></div><br/>
<br/>
Ещё есть прекрасные <a href="https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e">сиамские</a> <a href="https://sorenbouma.github.io/blog/oneshot/">сети</a>, однако их я, честно, сам не использовал. Кстати, “решает” не только сам лосс, а то, как для него семплировать пары positive’ов и negative’ов, это подчёркивают авторы статьи <a href="https://arxiv.org/abs/1706.07567">Sampling matters in deep embedding learning</a>.<br/>
<br/>
Суть всех этих лоссов и сиамских сетей проста — хотим, чтобы картинки одного класса (человека) в латентном пространстве фич (эмбеддингов) были “близко”, а разных классов (людей) — “далеко”. Близость обычно меряется так: берутся эмбеддинги картинок из нейросети (например, вектор из 128 чисел) и либо считаем обычное <a href="https://ru.wikipedia.org/wiki/%D0%95%D0%B2%D0%BA%D0%BB%D0%B8%D0%B4%D0%BE%D0%B2%D0%B0_%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B0">Евклидово расстояние</a> между этими векторами, либо <a href="https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C">косинусную близость.</a> Как именно мерить — лучше подбирать на своём датасете/задаче.<br/>
<br/>
Схематично пайплайн решения задач на representation learning выглядит примерно так:<br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/16/uh/n8/16uhn8l_iahuy-bcv_e4vohx1je.png" data-width="850"/></div><br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Но если быть более точным, то вот так</b><div class="spoiler_text"><b>На стадии обучения</b>: обучаем нейросеть либо на классификацию (Softmax + CrossEntropy), либо с помощью специального лосса (Triplet, Contrastive, etc.). Во втором случае ещё нужно правильно подбирать positive'ы и negative'ы в каждом батче<br/>
<br/>
<b>На стадии предсказания</b>: если это был именно какой-то особый лосс по типу триплета, то он на вход принимал уже эмбеддинги — их и берём. Если была классификация, то тут нужно экспериментировать — можно брать фичи с какого-то из свёрточных слоёв, а можно и вероятности после классификатора (да, так делают и это <i>работает</i>). Далее ищем расстояние от пришедшей в тесте картинки до всех картинок из галереи и выдаём метку ближайшей. Расстояние меряем <a href="https://en.wikipedia.org/wiki/Cosine_similarity">косинусом</a> или <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Евклидовой метрикой</a><br/>
</div></div><br/>
<br/>
Конкретно по <b>распознаванию лиц</b> есть несколько хороших статей: <a href="https://arxiv.org/abs/1804.06655">статья-обзор (<b>MUST READ!</b>)</a>, <a href="https://arxiv.org/abs/1503.03832">FaceNet</a>, <a href="https://arxiv.org/pdf/1801.07698.pdf">ArcFace</a>, <a href="https://arxiv.org/pdf/1801.09414.pdf">CosFace</a>.<br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" alt="image" data-src="https://habrastorage.org/getpro/habr/post_images/208/1b9/d34/2081b9d346f74503302b8fd2c7265ef5.png" data-width="700"/></div><br/>
<br/>
Реализаций тоже немало: <a href="http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html">dlib</a>, <a href="https://cmusatyalab.github.io/openface/">OpenFace</a>, <a href="https://github.com/davidsandberg/facenet">FaceNet repo</a>, да и <a href="https://habr.com/ru/post/317798/">на Хабре про это уже давно было рассказано</a>. Кажется, за последнее время добавились только ArcFace и CosFace (пишите в комментарии, если я здесь что-то упустил, буду рад узнать что-то ещё).<br/>
<br/>
Однако сейчас больше мода не на распознавание лиц, а на их <a href="https://thispersondoesnotexist.com/">генерацию</a>, не так ли?<br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/t9/k3/zv/t9k3zvmuf30yzmcvlube_okh5ey.png" data-width="500"/></div><br/>
В свою очередь, в задаче <b>ре-идентификации</b> сейчас бурная активность, статьи выходят каждый месяц, люди пробуют разные подходы, что-то работает уже сейчас, что-то пока ещё не очень. <br/>
<br/>
<img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/f_/pe/cd/f_pecd2dvv5kbatdpj0nkk4aapm.png"/><br/>
<br/>
Поясню суть задачи Reid на примере: есть галерея с <abbr title="вырезанная из большой фотографии детекция человка">кропами</abbr> людей, например, 10 людей, у каждого по 5 кропов (могут быть с разных сторон), то есть 50 фотографий в галерее. Приходит новая детекция (кроп), и надо сказать, какой это человек из галереи или сказать, что его там нет и завести для него новый ID. Задача усложняется тем, что детекции человека приходят с разных ракурсов: спереди, сзади, сбоку, <s>снизу</s>, и плюс камеры, с которых фото приходят, тоже разные (разные освещения/балансы белого и т.д.).<br/>
<br/>
<div style="text-align:center;"><img src="/img/image-loader.svg" data-src="https://habrastorage.org/webt/ca/pn/gr/capngrtiskbeltdx0oq_wfntw0i.png" data-width="700"/></div><br/>
<br/>
К слову, в нашей <a href="http://neuruslab.ru/">лаборатории</a> Reid — одна из ключевых задач. Статей выходит действительно немало, какие-то из них про новый более эффективный лосс, какие-то только про новый способ добычи negative'ов и positive'ов.<br/>
<br/>
Хороший обзор старых методов по Reid есть в <a href="https://arxiv.org/abs/1610.02984">статье 2016 года</a>. Сейчас, как я уже писал выше, применяются два подхода — классификация или representation learning. Однако есть специфика задачи, с ней исследователи борются по-разному, например, авторы <a href="https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch">Aligned Re-Id</a> предложили специальным образом выравнивать фичи (да, они смогли улучшить сеть с помощью динамического программирования<s>, Карл</s>), в <a href="https://arxiv.org/pdf/1711.10295.pdf">другой статье</a> предложили применить <a href="https://youtu.be/2t05gq13xy0">Generative Adversarial Networks (GAN)</a>.<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Есть ещё несколько трюков</b><div class="spoiler_text"><ul>
<li>Добавить новый тип аугментации, <a href="https://arxiv.org/pdf/1708.04896.pdf">закрывая случайные участки картинки</a></li>
<li><a href="https://arxiv.org/pdf/1604.07807v2.pdf">Слить вместе handcrafted-фичи и фичи из сетки</a></li>
<li><a href="https://arxiv.org/abs/1803.11333">Учесть разницу в ракурсах камер явно при обучении</a></li>
<li><a href="https://arxiv.org/pdf/1611.05244.pdf">Правильно делать Transfer Learning с одного датасета на другой</a></li>
</ul><br/>
</div></div><br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" alt="image" width="650" data-src="https://habrastorage.org/getpro/habr/post_images/456/66e/9c0/45666e9c0608373c31452aeb6a197477.jpg" data-blurred="true"/></div><br/>
<br/>
<div class="spoiler"><b class="spoiler_title">Демотиватор</b><div class="spoiler_text">Несмотря на все эти продвинутые методы, в моих экспериментах, почему-то, лучше всего себя показал именно подход с классификацией. Возможно, я что-то не учёл, но пока что немного грустно, что придумали столько всего, а в итоге работает <s>старая добрая логистическая регрессия</s> классификация. Но главное — пробовать и не сдаваться!<br/>
</div></div><br/>
<br/>
Из реализаций хочется обязательно упомянуть <a href="https://github.com/Cysu/open-reid">OpenReid</a> и <a href="https://github.com/KaiyangZhou/deep-person-reid">TorchReid</a>. Обратите внимание на сам код — на мой взгляд, он написан грамотно с точки зрения архитектуры фреймворка, подробнее <a href="https://cysu.github.io/open-reid/notes/overview.html">здесь</a>. Плюс они оба на PyTorch, и в Readme есть много ссылок на статьи по Person Re-identification, это приятно.<br/>
<br/>
Вообще особый спрос на face- и reid-алгоритмы сейчас в Китае (<a href="https://meduza.io/feature/2018/02/11/v-kitae-sozdayut-totalnuyu-sistemu-raspoznavaniya-lits-grazhdan-ona-pomozhet-lovit-prestupnikov-i-sobirat-dannye-na-vseh-ostalnyh">если вы понимаете, о чём я</a>). Мы на очереди? Кто знает…<br/>
<br/>
<h3>Слово про ускорение нейросетей</h3><br/>
Мы уже говорили о том, что можно просто придумать легковесную архитектуру. Но как быть, если сеть уже обучена и она крута, а сжать её всё равно нужно? В таком случае может помочь один (или все) из следующих методов:<br/>
<br/>
<ul>
<li>Дистилляция: <a href="https://arxiv.org/abs/1503.02531">раз</a>, <a href="https://nervanasystems.github.io/distiller/knowledge_distillation.html">два</a>, <a href="https://medium.com/neural-machines/knowledge-distillation-dc241d7c2322">три</a></li>
<li>Квантизация: <a href="https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd">раз</a>, <a href="https://nervanasystems.github.io/distiller/quantization.html">два</a></li>
<li>Прунинг: <a href="https://habr.com/ru/post/413939/">раз</a>, <a href="https://jacobgil.github.io/deeplearning/pruning-deep-learning">два</a></li>
</ul><br/>
Ну и правило использовать не float64, а, например, float32 никто не отменял. Есть даже свежая <a href="https://arxiv.org/abs/1904.11943">статья про low-precision training</a>. Недавно, кстати, Google представил <a href="https://ai.googleblog.com/2019/04/morphnet-towards-faster-and-smaller.html">MorphNet</a>, которая (вроде как) помогает автоматически сжимать модель.<br/>
<br/>
<a name="7"></a><h3>А что дальше?</h3><br/>
<br/>
<div style="text-align:center;"><img src="https://habrastorage.org/r/w780q1/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" alt="image" width="500" data-src="https://habrastorage.org/webt/ue/22/e1/ue22e11md3zjexlxq3jxsf-kx18.jpeg" data-blurred="true"/></div><br/>
<br/>
Мы обсудили действительно много полезных и прикладных вещей в DL и CV: классификация, архитектуры сетей, визуализация, эмбеддинги. Однако в современном зрении есть ещё и другие важные задачи: детектирование, сегментация, понимание сцены. Если речь про видео, то хочется объекты <a href="https://ru.wikipedia.org/wiki/%D0%A2%D1%80%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3_(%D0%BA%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D0%B0%D1%8F_%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%B0)">трекать во времени</a>, распознавать действия и понимать, что на видео происходит. Именно этим вещам и будет посвящена вторая часть обзора.<br/>
<br/>
Stay tuned!<br/>
<br/>
<div class="spoiler"><b class="spoiler_title">P. S.: Какое образование сейчас предлагает Физтех-школа ПМИ МФТИ?</b><div class="spoiler_text">ФПМИ МФТИ организует программу бакалавриата (кстати, теперь и <a href="http://cs-mipt.ru/">на английском</a>, для иностранных студентов), однако сейчас активно развиваются и <a href="https://mipt.ru/education/departments/fpmi/master/">магистерские программы</a>, причём как очные, так и <a href="http://omscmipt.ru/">онлайн</a>. Со всем списком образовательных возможностей ФПМИ МФТИ можно ознакомиться <a href="https://www.dlschool.org/fpmi">здесь</a>. Если вам интересно, буду рад обсудить конкретные программы в комментариях или личных сообщениях, а то ведь <a href="https://music.yandex.ru/album/5237771/track/40321941">Физтех уже не тот</a> (в хорошем смысле).<br/>
</div></div></div></div> <!----> <!----></div> <div class="tm-article-presenter__meta"><div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Теги:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%B8%D0%B8%5D" class="tm-tags-list__link">ии</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%B3%D0%BB%D1%83%D0%B1%D0%BE%D0%BA%D0%BE%D0%B5%20%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%5D" class="tm-tags-list__link">глубокое обучение</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BA%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D0%BE%D0%B5%20%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5%5D" class="tm-tags-list__link">компьютерное зрение</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B9%5D" class="tm-tags-list__link">обучение представлений</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%B2%D0%B8%D0%B7%D1%83%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%20%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B5%D0%B9%5D" class="tm-tags-list__link">визуализация нейросетей</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D1%8B%D0%B5%20%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D1%81%D0%B5%D1%82%D0%B8%5D" class="tm-tags-list__link">сверточные нейросети</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bcomputer%20vIsion%5D" class="tm-tags-list__link">computer vIsion</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bmetric%20learning%5D" class="tm-tags-list__link">metric learning</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5%20%D1%81%D0%B5%D1%82%D0%B8%5D" class="tm-tags-list__link">нейронные сети</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5Bdeep%20learning%20school%5D" class="tm-tags-list__link">deep learning school</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D1%84%D0%BF%D0%BC%D0%B8%5D" class="tm-tags-list__link">фпми</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%BC%D1%84%D1%82%D0%B8%5D" class="tm-tags-list__link">мфти</a></li><li class="tm-separated-list__item"><a href="/ru/search/?target_type=posts&amp;order=relevance&amp;q=%5B%D0%A4%D0%B8%D0%B7%D1%82%D0%B5%D1%85%5D" class="tm-tags-list__link">Физтех</a></li></ul></div> <div class="tm-separated-list tm-article-presenter__meta-list"><span class="tm-separated-list__title">Хабы:</span> <ul class="tm-separated-list__list"><li class="tm-separated-list__item"><a href="/ru/company/mipt/blog/" class="tm-hubs-list__link router-link-active">
    Блог компании Московский физико-технический институт (МФТИ)
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/programming/" class="tm-hubs-list__link">
    Программирование
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/image_processing/" class="tm-hubs-list__link">
    Обработка изображений
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/machine_learning/" class="tm-hubs-list__link">
    Машинное обучение
  </a></li><li class="tm-separated-list__item"><a href="/ru/hub/artificial_intelligence/" class="tm-hubs-list__link">
    Искусственный интеллект
  </a></li></ul></div></div></article></div> <!----></div> <div class="tm-article-sticky-panel"><div class="tm-data-icons tm-article-sticky-panel__icons"><div class="tm-article-rating tm-data-icons__item"><div class="tm-votes-meter tm-article-rating__votes-switcher"><svg height="16" width="16" class="tm-svg-img tm-votes-meter__icon tm-votes-meter__icon_medium"><title>Всего голосов 18: ↑17 и ↓1</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-rating"></use></svg> <span title="Всего голосов 18: ↑17 и ↓1" class="tm-votes-meter__value tm-votes-meter__value_positive tm-votes-meter__value_medium">+16</span></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----> <span title="Количество просмотров" class="tm-icon-counter tm-data-icons__item"><svg height="16" width="16" class="tm-svg-img tm-icon-counter__icon"><title>Просмотры</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-views"></use></svg> <span class="tm-icon-counter__value">19K</span></span> <button title="Добавить в закладки" type="button" class="bookmarks-button tm-data-icons__item"><span title="Добавить в закладки" class="tm-svg-icon__wrapper bookmarks-button__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Добавить в закладки</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-favorite"></use></svg></span> <span title="Количество пользователей, добавивших публикацию в закладки" class="bookmarks-button__counter">
    164
  </span></button> <!----> <div title="Поделиться" class="tm-sharing tm-data-icons__item"><button type="button" class="tm-sharing__button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="tm-sharing__icon"><path fill="currentColor" d="M10.33.275l9.047 7.572a.2.2 0 010 .306l-9.048 7.572a.2.2 0 01-.328-.153V11c-8 0-9.94 6-9.94 6S-1 5 10 5V.428a.2.2 0 01.328-.153z"></path></svg></button> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> </div></div> <!----> <!----> <div class="tm-article-presenter__footer"><div class="tm-article-blocks"><!----> <section class="tm-block tm-block_spacing-bottom"><!----> <div class="tm-block__body tm-block__body_variant-balanced"><div class="tm-article-author"><div class="tm-article-author__company"><div class="tm-article-author__company-card"><div class="tm-company-snippet"><a href="/ru/company/mipt/profile/" class="tm-company-snippet__logo-link"><div class="tm-entity-image"><img alt="" height="40" src="//habrastorage.org/getpro/habr/company/70d/544/daf/70d544daf4932b17afc9dd3efa0a9cf3.jpg" width="40" class="tm-entity-image__pic"></div></a> <div class="tm-company-snippet__info"><a href="/ru/company/mipt/profile/" class="tm-company-snippet__title">Московский физико-технический институт (МФТИ)</a> <div class="tm-company-snippet__description">Компания</div></div></div> <div class="tm-article-author__buttons"><!----> <!----></div></div> <div class="tm-article-author__company-contacts"><a href="https://facebook.com/MIPT.rus" rel="noopener" target="_blank" class="tm-article-author__contact">
      Facebook
    </a><a href="https://twitter.com/miptru" rel="noopener" target="_blank" class="tm-article-author__contact">
      Twitter
    </a><a href="https://vk.com/miptru" rel="noopener" target="_blank" class="tm-article-author__contact">
      ВКонтакте
    </a><a href="https://instagram.com/mipt.ru" rel="noopener" target="_blank" class="tm-article-author__contact">
      Instagram
    </a></div> <div class="tm-article-author__separator"></div></div> <div class="tm-user-card tm-article-author__user-card tm-user-card_variant-article"><div class="tm-user-card__info-container"><div class="tm-user-card__header"><div class="tm-user-card__header-data"><a href="/ru/users/izakharkin/" class="tm-user-card__userpic tm-user-card__userpic_size-40"><div class="tm-entity-image"><img alt="" src="//habrastorage.org/getpro/habr/avatars/aa8/0aa/da7/aa80aada722e56f3beec6ac3c651de4a.jpg" class="tm-entity-image__pic"></div></a> <div class="tm-user-card__meta"><div title=" 48 голосов " class="tm-karma tm-user-card__karma"><div class="tm-karma__votes tm-karma__votes_positive">
    46
  </div> <div class="tm-karma__text">
    Карма
  </div></div> <div title="Рейтинг пользователя" class="tm-rating tm-user-card__rating"><div class="tm-rating__header"> <div class="tm-rating__counter">0</div></div> <div class="tm-rating__text">
    Рейтинг
  </div></div></div></div></div> <div class="tm-user-card__info tm-user-card__info_variant-article"><div class="tm-user-card__title tm-user-card__title_variant-article"><span class="tm-user-card__name tm-user-card__name_variant-article">Илья Захаркин</span> <a href="/ru/users/izakharkin/" class="tm-user-card__nickname tm-user-card__nickname_variant-article">
          @izakharkin
        </a> <!----></div> <p class="tm-user-card__short-info tm-user-card__short-info_variant-article">Computer Vision & Graphics RnD</p></div></div> <div class="tm-user-card__buttons tm-user-card__buttons_variant-article"><!----> <!----> <!----> <!----> <!----></div></div> <!----></div> <DIV class="v-portal" style="display:none;"></DIV></div> <!----></section> <div class="tm-article-blocks__comments"><div class="tm-article-page-comments"><div class="tm-article-comments-counter-link tm-article-comments-counter-button"><a href="/ru/company/mipt/blog/450732/comments/" class="tm-article-comments-counter-link__link tm-article-comments-counter-link__link_button-style"><svg height="16" width="16" class="tm-svg-img tm-article-comments-counter-link__icon tm-article-comments-counter-link__icon_contrasted"><title>Комментарии</title> <use xlink:href="/img/megazord-v24.ce74655c.svg#counter-comments"></use></svg> <span class="tm-article-comments-counter-link__value tm-article-comments-counter-link__value_contrasted">
       Комментарии 5 
    </span></a> <!----></div></div></div>  <!---->  <!----> <!----></div></div></div></div></div> <div class="tm-page__sidebar"><div class="tm-layout-sidebar"><div class="tm-layout-sidebar__placeholder_initial"></div> <div class="tm-sexy-sidebar tm-sexy-sidebar_initial" style="margin-top:0px;"><!----> <section class="tm-block tm-block_spacing-bottom"><header class="tm-block__header"><h2 class="tm-block__title">Информация</h2> <!----></header> <div class="tm-block__body"><div class="tm-company-basic-info"><dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата основания</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="1946-11-24T21:00:00.000Z" title="1946-11-25, 00:00">25  ноября  1946</time></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Местоположение</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    Россия
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Сайт</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><a href="http://mipt.ru" target="_blank" class="tm-company-basic-info__link">
      mipt.ru
    </a></dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Численность</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap">
    1 001–5 000 человек
  </dd></dl> <dl class="tm-description-list tm-description-list_variant-columns-nowrap"><dt class="tm-description-list__title tm-description-list__title_variant-columns-nowrap">Дата регистрации</dt> <dd class="tm-description-list__body tm-description-list__body_variant-columns-nowrap"><time datetime="2014-01-21T06:06:00.000Z" title="2014-01-21, 10:06">21  января  2014</time></dd></dl> <!----></div></div> <!----></section> <!----> <!----></div></div></div></div></div></div></main> <!----></div> <div class="tm-footer-menu"><div class="tm-page-width"><div class="tm-footer-menu__container"><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Ваш аккаунт
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr/?back=/ru/company/mipt/blog/450732/&amp;hl=ru" rel="nofollow" target="_self">
                Войти
              </a></li><li class="tm-footer-menu__list-item"><a href="/kek/v1/auth/habrahabr-register/?back=/ru/company/mipt/blog/450732/&amp;hl=ru" rel="nofollow" target="_self">
                Регистрация
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Разделы
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/" class="footer-menu__item-link router-link-active">
                Публикации
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/news/" class="footer-menu__item-link">
                Новости
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/hubs/" class="footer-menu__item-link">
                Хабы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/companies/" class="footer-menu__item-link">
                Компании
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/users/" class="footer-menu__item-link">
                Авторы
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/sandbox/" class="footer-menu__item-link">
                Песочница
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Информация
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="/ru/docs/help/" class="footer-menu__item-link">
                Устройство сайта
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/authors/codex/" class="footer-menu__item-link">
                Для авторов
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/companies/corpblogs/" class="footer-menu__item-link">
                Для компаний
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/docs/docs/transparency/" class="footer-menu__item-link">
                Документы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/agreement" target="_blank">
                Соглашение
              </a></li><li class="tm-footer-menu__list-item"><a href="https://account.habr.com/info/confidential/" target="_blank">
                Конфиденциальность
              </a></li></ul></div></div><div class="tm-footer-menu__block"><h3 class="tm-footer-menu__block-title">
          Услуги
        </h3> <div class="tm-footer-menu__block-content"><ul class="tm-footer-menu__list"><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQLwRfQmXibiUlWaRg-BAc38s7oM3lJiaPju7qmdJsp8ysIvZ_G-Npem0njJLMozE2bPHMpDqiI5hhy/pub?start=false&amp;loop=false&amp;delayms=60000&amp;slide=id.g91a03369cd_4_297" target="_blank">
                Реклама
              </a></li><li class="tm-footer-menu__list-item"><a href="https://habrastorage.org/storage/stuff/habr/service_price.pdf" target="_blank">
                Тарифы
              </a></li><li class="tm-footer-menu__list-item"><a href="https://docs.google.com/presentation/d/e/2PACX-1vQJJds8-Di7BQSP_guHxICN7woVYoN5NP_22ra-BIo4bqnTT9FR6fB-Ku2P0AoRpX0Ds-LRkDeAoD8F/pub?start=false&amp;loop=false&amp;delayms=60000" target="_blank">
                Контент
              </a></li><li class="tm-footer-menu__list-item"><a href="https://tmtm.timepad.ru/" target="_blank">
                Семинары
              </a></li><li class="tm-footer-menu__list-item"><a href="/ru/megaprojects/" class="footer-menu__item-link">
                Мегапроекты
              </a></li></ul></div></div></div></div></div> <div class="tm-footer"><div class="tm-page-width"><div class="tm-footer__container"><!----> <div class="tm-footer__social"><a href="https://www.facebook.com/habrahabr.ru" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Facebook</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-facebook"></use></svg></a><a href="https://twitter.com/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Twitter</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-twitter"></use></svg></a><a href="https://vk.com/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>VK</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-vkontakte"></use></svg></a><a href="https://telegram.me/habr_com" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Telegram</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-telegram"></use></svg></a><a href="https://www.youtube.com/channel/UCd_sTwKqVrweTt4oAKY5y4w" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Youtube</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-youtube"></use></svg></a><a href="https://zen.yandex.ru/habr" rel="nofollow noopener noreferrer" target="_blank" class="tm-svg-icon__wrapper tm-social-icons__icon"><svg height="16" width="16" class="tm-svg-img tm-svg-icon"><title>Яндекс Дзен</title> <use xlink:href="/img/social-icons-sprite.svg#social-logo-zen"></use></svg></a></div> <DIV class="v-portal" style="display:none;"></DIV> <button class="tm-footer__link"><!---->
        Настройка языка
      </button> <a href="/ru/about" class="tm-footer__link">
        О сайте
      </a> <a href="/ru/feedback/" class="tm-footer__link">
        Техническая поддержка
      </a> <!----> <a href="/berserk-mode-nope" class="tm-footer__link">
        Вернуться на старую версию
      </a> <div class="tm-footer-copyright"><span class="tm-copyright"><span class="tm-copyright__years">© 2006–2021 </span> <span class="tm-copyright__name">«<a href="https://company.habr.com/" rel="noopener" target="_blank" class="tm-copyright__link">Habr</a>»</span></span></div></div></div></div> <!----> <!----></div> <div class="vue-portal-target"></div></div>
<script>window.__INITIAL_STATE__={"adblock":{"hasAcceptableAdsFilter":false,"hasAdblock":false},"articlesList":{"articlesList":{"450732":{"id":"450732","timePublished":"2019-05-21T11:31:04+00:00","isCorporative":true,"lang":"ru","titleHtml":"Вижу, значит существую: обзор Deep Learning в Computer Vision (часть 1)","leadData":{"textHtml":"Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами. А ведь их очень много, и они правда крутые! Если вам интересно, что сейчас происходит в области Computer Vision не только с точки зрения \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Flist\u002Fcs.CV\u002FRecent\"\u003Eисследований и статей\u003C\u002Fa\u003E, но и с точки зрения прикладных задач, то милости прошу под кат. Также статья может стать неплохим введением для тех, кто давно хотел начать разбираться во всём этом, но что-то мешало ;)\u003Cbr\u003E\r\n\u003Cbr\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fecb\u002F319\u002Fe06\u002Fecb319e06d692a5ea4f2a1343cf9c31d.jpg\" alt=\"image\"\u003E\u003Cbr\u003E","imageUrl":null,"buttonTextHtml":"Читать дальше →","image":null},"editorVersion":"1.0","postType":"article","postLabels":[],"author":{"scoreStats":{"score":46,"votesCount":48},"rating":0,"relatedData":null,"contacts":[],"authorContacts":[],"paymentDetails":{"paymentYandexMoney":null,"paymentPayPalMe":null,"paymentWebmoney":null},"id":"1264680","alias":"izakharkin","fullname":"Илья Захаркин","avatarUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Favatars\u002Faa8\u002F0aa\u002Fda7\u002Faa80aada722e56f3beec6ac3c651de4a.jpg","speciality":"Computer Vision & Graphics RnD"},"statistics":{"commentsCount":5,"favoritesCount":164,"readingCount":18753,"score":16,"votesCount":18},"hubs":[{"relatedData":null,"id":"18808","alias":"mipt","type":"corporative","title":"Блог компании Московский физико-технический институт (МФТИ)","titleHtml":"Блог компании Московский физико-технический институт (МФТИ)","isProfiled":false},{"relatedData":null,"id":"359","alias":"programming","type":"collective","title":"Программирование","titleHtml":"Программирование","isProfiled":true},{"relatedData":null,"id":"17175","alias":"image_processing","type":"collective","title":"Обработка изображений","titleHtml":"Обработка изображений","isProfiled":true},{"relatedData":null,"id":"19439","alias":"machine_learning","type":"collective","title":"Машинное обучение","titleHtml":"Машинное обучение","isProfiled":true},{"relatedData":null,"id":"21922","alias":"artificial_intelligence","type":"collective","title":"Искусственный интеллект","titleHtml":"Искусственный интеллект","isProfiled":false}],"flows":[{"id":"1","alias":"develop","title":"Разработка"},{"id":"7","alias":"popsci","title":"Научпоп"}],"relatedData":null,"textHtml":"\u003Cdiv xmlns=\"http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxhtml\"\u003EКомпьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами. А ведь их очень много, и они правда крутые! Если вам интересно, что сейчас происходит в области Computer Vision не только с точки зрения \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Flist\u002Fcs.CV\u002FRecent\"\u003Eисследований и статей\u003C\u002Fa\u003E, но и с точки зрения прикладных задач, то милости прошу под кат. Также статья может стать неплохим введением для тех, кто давно хотел начать разбираться во всём этом, но что-то мешало ;)\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fecb\u002F319\u002Fe06\u002Fecb319e06d692a5ea4f2a1343cf9c31d.jpg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fecb\u002F319\u002Fe06\u002Fecb319e06d692a5ea4f2a1343cf9c31d.jpg\" data-blurred=\"true\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Ca name=\"habracut\"\u003E\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\r\nСегодня на Физтехе происходит активная коллаборация «академии» и индустриальных партнёров. В частности, в \u003Ca href=\"https:\u002F\u002Fmipt.ru\u002Feducation\u002Fdepartments\u002Ffpmi\u002F\"\u003EФизтех-школе Прикладной математики и информатики\u003C\u002Fa\u003E действуют множество \u003Ca href=\"https:\u002F\u002Fmipt.ru\u002Feducation\u002Fdepartments\u002Ffpmi\u002Flabs\u002F\"\u003Eинтересных лабораторий\u003C\u002Fa\u003E от таких компаний, как Сбербанк, Biocad, 1С, Тинькофф, МТС, Huawei. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nНа написание этой статьи меня вдохновила работа в Лаборатории \u003Ca href=\"http:\u002F\u002Fneuruslab.ru\u002F\"\u003Eгибридных интеллектуальных систем\u003C\u002Fa\u003E, открытой компанией \u003Ca href=\"https:\u002F\u002Fnew.vkusvill.ru\u002F\"\u003EВкусВилл\u003C\u002Fa\u003E. У лаборатории амбициозная задача — построить магазин, работающий без касс, в основном при помощи компьютерного зрения. За почти год работы мне довелось поработать над многими задачами зрения, о которых и пойдёт речь в этих двух частях.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EМагазин без касс? Где-то я это уже слышал..\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EНаверное, дорогой читатель, Вы подумали про \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F399725\u002F\"\u003EAmazon Go\u003C\u002Fa\u003E. В каком-то смысле стоит задача повторить их успех, однако наше решение больше про внедрение, нежели про построение такого магазина с нуля за \u003Ca href=\"https:\u002F\u002Fwww.bloomberg.com\u002Fnews\u002Farticles\u002F2018-09-20\u002Famazon-could-spend-3-billion-on-go-stores-analyst-says\"\u003Eогромные деньги\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nБудем двигаться по плану:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Col\u003E\r\n\u003Cli\u003E\u003Ca href=\"#1\"\u003EМотивация и что вообще происходит\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"#2\"\u003EКлассификация как стиль жизни\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"#3\"\u003EАрхитектуры свёрточных нейросетей: 1000 способов достичь одной цели \u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"#4\"\u003EВизуализация свёрточных нейросетей: покажи мне страсть\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"#5\"\u003EЯ и сам своего рода хирург: извлекаем фичи из нейросетей\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"#6\"\u003EДержись рядом: representation learning для людей и лиц\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"#7\"\u003EЧасть 2: \u003Cs\u003Eдетектирование, оценка позы и распознавание действий\u003C\u002Fs\u003E без спойлеров\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003C\u002Fol\u003E\u003Cbr\u002F\u003E\r\n\u003Ca name=\"1\"\u003E\u003C\u002Fa\u003E\u003Ch2\u003EМотивация и что вообще происходит\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EДля кого статья?\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EСтатья ориентирована в большей степени на людей, которые уже знакомы с машинным обучением и нейросетями. Однако советую прочитать хотя бы первые два раздела — вдруг всё будет понятно :)\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nВ 2019 году все говорят про искусственный интеллект, \u003Ca href=\"https:\u002F\u002Fmeduza.io\u002Fcards\u002Fekonomisty-obsuzhdayut-chetvertuyu-promyshlennuyu-revolyutsiyu-chto-eto\"\u003Eчетвёртую промышленную революцию\u003C\u002Fa\u003E и \u003Ca href=\"https:\u002F\u002Fsocialego.mediasole.ru\u002Frey_kurcveyl_raspisal_buduschee_mira_prognoz_do_2099_goda\"\u003Eприближение человечества к сингулярности\u003C\u002Fa\u003E. Круто, классно, но хочется конкретики. Ведь мы с вами любопытные технари, которые не верят в сказки про ИИ, мы верим в формальную постановку задач, математику и программирование. В этой статье мы поговорим о конкретных кейсах применения того самого современного ИИ — о применении deep learning (а именно — свёрточных нейросетей) в множестве задач компьютерного зрения. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nДа, мы будем говорить именно про сетки, иногда упоминая некоторые идеи из «классического» зрения (так будем называть набор методов в зрении, которые использовались до нейросетей, однако это ни в коем случае не значит, что сейчас они не используются). \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EХочу изучить компьютерное зрение с нуля\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EРекомендую \u003Ca href=\"https:\u002F\u002Fwww.lektorium.tv\u002Fcourse\u002F22847\"\u003Eкурс Антона Конушина «Введение в компьютерное зрение»\u003C\u002Fa\u003E. Лично я проходил его аналог в ШАДе, что заложило прочную основу в понимании обработки изображений и видео.\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fgu\u002Fvu\u002Fo3\u002Fguvuo3vejwwjimlpcqiwgbpxldq.jpeg\" alt=\"image\" width=\"300\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fgu\u002Fvu\u002Fo3\u002Fguvuo3vejwwjimlpcqiwgbpxldq.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nНа мой взгляд, первое действительно интересное применение нейросетей в зрении, которое было освещено в СМИ ещё в 1993 году, это \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=FwFduRA_L6Q&amp;feature=youtu.be\"\u003Eраспознавание рукописных цифр\u003C\u002Fa\u003E, \u003Ca href=\"http:\u002F\u002Fyann.lecun.com\u002Fexdb\u002Fpublis\u002Fpdf\u002Flecun-90c.pdf\"\u003Eреализованное Яном ЛеКуном\u003C\u002Fa\u003E. Сейчас он один из главных по ИИ в \u003Ca href=\"https:\u002F\u002Fresearch.fb.com\u002Fcategory\u002Ffacebook-ai-research\u002F\"\u003EFacebook AI Research\u003C\u002Fa\u003E, их команда выпустила уже \u003Ca href=\"https:\u002F\u002Fopensource.facebook.com\u002F#artificial-intelligence\"\u003Eнемало полезных Open Source вещей\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nСегодня же зрение применяется во многих сферах. Приведу лишь несколько ярких примеров: \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002F3x\u002Ftl\u002F-j\u002F3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg\" alt=\"image\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F3x\u002Ftl\u002F-j\u002F3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F967\u002F987\u002F50c\u002F96798750c04282d6514f994b8375edcb.jpg\" alt=\"image\" width=\"400\" height=\"300\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F967\u002F987\u002F50c\u002F96798750c04282d6514f994b8375edcb.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EБеспилотные автомобили \u003Ca href=\"https:\u002F\u002Fwww.tesla.com\u002Fautopilot\"\u003ETesla\u003C\u002Fa\u003E и \u003Ca href=\"https:\u002F\u002Fyandex.ru\u002Fpromo\u002Ftaxi\u002Fsdc\"\u003EЯндекса\u003C\u002Fa\u003E\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fdda\u002F997\u002F082\u002Fdda9970829bfb17bb2b118a08d519835.jpg\" alt=\"image\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fdda\u002F997\u002F082\u002Fdda9970829bfb17bb2b118a08d519835.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003E\u003Ca href=\"https:\u002F\u002Facademic.oup.com\u002Fjnci\u002Fadvance-article-abstract\u002Fdoi\u002F10.1093\u002Fjnci\u002Fdjy222\u002F5307077?redirectedFrom=fulltext\"\u003EАнализ медицинских снимков\u003C\u002Fa\u003E и \u003Ca href=\"https:\u002F\u002Fwww.nature.com\u002Farticles\u002Fs41591-018-0177-5.epdf?referrer_access_token=Vcd6TxYnYcWaq1GBsBATUdRgN0jAjWel9jnR3ZoTv0NHburD9_XZ7WMEhIlD9p3NpWLpsg4XdGg7RALozn59eUhEzW5MtONSBjWG2OZXXgEmuDAUdQU8Uba-wwWSDO7SEqELRaDpZA4VgrIZd-yoJVelT9hlYSARgD6lX1MUs8Pfx2qBop3sw39xD2kYf-mO&amp;tracking_referrer=www.wired.com\"\u003Eпредсказание рака\u003C\u002Fa\u003E\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fjz\u002F9k\u002F2o\u002Fjz9k2ovcurxg4zd_cj_kb20hs_0.jpeg\" alt=\"image\" width=\"500\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fjz\u002F9k\u002F2o\u002Fjz9k2ovcurxg4zd_cj_kb20hs_0.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EИгровые приставки: Kinect 2.0 (правда, там ещё используется информация о глубине, то есть RGB-D картинки)\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F4d1\u002Ffb8\u002F125\u002F4d1fb8125d4624b40993f441b42ac48d.jpg\" alt=\"image\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F4d1\u002Ffb8\u002F125\u002F4d1fb8125d4624b40993f441b42ac48d.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fwf\u002Fkw\u002Fla\u002Fwfkwlap8pltophsuh1ggkxgkii8.jpeg\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fwf\u002Fkw\u002Fla\u002Fwfkwlap8pltophsuh1ggkxgkii8.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EРаспознавание по лицу: \u003Ca href=\"https:\u002F\u002Fsupport.apple.com\u002Fru-ru\u002FHT208109\"\u003EApple FaceID\u003C\u002Fa\u003E (при помощи нескольких датчиков)\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F2d3\u002Ff3b\u002F178\u002F2d3f3b17818ae279e7a47d3c940e002f.jpg\" alt=\"image\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F2d3\u002Ff3b\u002F178\u002F2d3f3b17818ae279e7a47d3c940e002f.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EОценка точек лица: \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=Pc2aJxnmzh0\"\u003Eмаски в Snapchat\u003C\u002Fa\u003E\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fhn\u002Fcw\u002Foc\u002Fhncwocoggiei8lkijpl8ihgbx_o.jpeg\" alt=\"image\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fhn\u002Fcw\u002Foc\u002Fhncwocoggiei8lkijpl8ihgbx_o.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EБиометрия лица и движений глаз (пример из \u003Ca href=\"https:\u002F\u002Fmipt.ru\u002Fscience\u002Flabs\u002Fmipt-sberbank-applied-research\u002Fprojects\u002Fnovaya_biometricheskaya_autentifikatsiya\"\u003Eпроекта ФПМИ МФТИ\u003C\u002Fa\u003E)\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fvg\u002Fvv\u002F4f\u002Fvgvv4f_ddwswudk1yvghxjl4rne.png\" data-width=\"400\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EПоиск по картинке: Яндекс и Google\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fb4d\u002Fcfd\u002Fd13\u002Fb4dcfdd13f85affc79d876cf4bd3f4fd.jpg\" alt=\"image\" width=\"500\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fb4d\u002Fcfd\u002Fd13\u002Fb4dcfdd13f85affc79d876cf4bd3f4fd.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EРаспознавание текста на картинке (\u003Ca href=\"https:\u002F\u002Fru.wikipedia.org\u002Fwiki\u002F%D0%9E%D0%BF%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%B8%D0%BC%D0%B2%D0%BE%D0%BB%D0%BE%D0%B2\"\u003EOptical Character Recognition\u003C\u002Fa\u003E)\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fcfa\u002F2bb\u002Fafa\u002Fcfa2bbafae96a5bd082ef25bae9d19af.jpg\" alt=\"image\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fcfa\u002F2bb\u002Fafa\u002Fcfa2bbafae96a5bd082ef25bae9d19af.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F60d\u002F62d\u002F670\u002F60d62d670999dcc7cbd726dde47905a0.jpg\" alt=\"image\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F60d\u002F62d\u002F670\u002F60d62d670999dcc7cbd726dde47905a0.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EДроны и роботы: получение и обработка информации с помощью зрения\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F113\u002F220\u002Fca0\u002F113220ca03176c5a99b82819076e0c8a.jpg\" alt=\"image\" width=\"500\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F113\u002F220\u002Fca0\u002F113220ca03176c5a99b82819076e0c8a.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Ci\u003E\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F404757\u002F\"\u003EОдометрия\u003C\u002Fa\u003E: построение карты и планирование при перемещении роботов\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fju\u002Fb7\u002Fi6\u002Fjub7i61z3oiairdg2q45x0l6loi.png\" data-width=\"500\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003E\u003Ca href=\"https:\u002F\u002Fevrl.to\u002Farticles\u002F5c1f4c056ec7f73533eac24b\u002Fkak-nejronnye-seti-uluchshajut-grafiku-v-staryh-igrah\u002F\"\u003EУлучшение графики и текстур в видеоиграх\u003C\u002Fa\u003E\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fd56\u002F155\u002F0ee\u002Fd561550eec9f5badc4475392a584fe03.jpg\" alt=\"image\" width=\"200\" height=\"300\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fd56\u002F155\u002F0ee\u002Fd561550eec9f5badc4475392a584fe03.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EПеревод по картинке: Яндекс и Google\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F859\u002Fffd\u002F2d5\u002F859ffd2d56f231c5f9b802978a688c94.jpg\" alt=\"image\" width=\"500\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F859\u002Fffd\u002F2d5\u002F859ffd2d56f231c5f9b802978a688c94.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F31f\u002F003\u002Fa47\u002F31f003a47c5dc5b5c5f75758d4d3689c.jpg\" alt=\"image\" width=\"500\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F31f\u002F003\u002Fa47\u002F31f003a47c5dc5b5c5f75758d4d3689c.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EДополненная реальность: \u003Ca href=\"https:\u002F\u002Fdeveloper.leapmotion.com\u002Fnorthstar\"\u003ELeap Motion (Project North Star)\u003C\u002Fa\u003E и \u003Ca href=\"https:\u002F\u002Fwww.microsoft.com\u002Fen-ca\u002Fhololens\"\u003EMicrosoft Hololens\u003C\u002Fa\u003E\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cp\u003E\u003C\u002Fp\u003E\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F9a4\u002F3ea\u002F74b\u002F9a43ea74ba0b5595f257feb313756293.jpg\" alt=\"image\" width=\"250\" height=\"200\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F9a4\u002F3ea\u002F74b\u002F9a43ea74ba0b5595f257feb313756293.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fe90\u002F962\u002F25b\u002Fe9096225bb7d5799823737c960e19ad6.jpg\" width=\"250\" height=\"300\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fe90\u002F962\u002F25b\u002Fe9096225bb7d5799823737c960e19ad6.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ci\u003EПеренос стиля и текстур: \u003Ca href=\"https:\u002F\u002Fprisma-ai.com\u002F\"\u003EPrisma\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fpicsart.com\u002F\"\u003EPicsArt\u003C\u002Fa\u003E\u003C\u002Fi\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЯ уже не говорю про многочисленные применения в различных внутренних задачах компаний. Facebook, к примеру, применяет зрение ещё и для того, чтобы фильтровать медиаконтент. В \u003Ca href=\"https:\u002F\u002Fblogs.nvidia.com\u002Fblog\u002F2019\u002F04\u002F18\u002Flucidyne-gradescan-lumber-grading\u002F\"\u003Eпроверке качества\u002Fповреждений в промышленности\u003C\u002Fa\u003E тоже используются методы компьютерного зрения.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nДополненной реальности здесь нужно, на самом деле, уделить отдельное внимание, поскольку \u003Cs\u003Eона не работает\u003C\u002Fs\u003E в скором времени это может стать одной из главных областей применения зрения.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nСмотивировались. Зарядились. Поехали:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ca name=\"2\"\u003E\u003C\u002Fa\u003E\u003Ch2\u003EКлассификация как стиль жизни\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fai2-s2-public.s3.amazonaws.com\u002Ffigures\u002F2017-08-08\u002F38211dc39e41273c0007889202c69f841e02248a\u002F2-Figure1-1.png\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nКак я уже говорил, в 90-е годы сетки в зрении выстрелили. Причём выстрелили в конкретной задаче — задаче классификации картинок рукописных цифр (знаменитый \u003Ca href=\"https:\u002F\u002Fwww.kaggle.com\u002Fc\u002Fdigit-recognizer\u002Fdata\"\u003Eдатасет MNIST\u003C\u002Fa\u003E). Исторически сложилось, что именно задача классификации изображений и стала основой при решении почти всех последующих задач в зрении. Рассмотрим конкретный пример:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003EЗадача\u003C\u002Fb\u003E: На вход дана папка с фотографиями, на каждом фото тот или иной объект: либо кошка, либо собака, либо человек (пусть “мусорных” фоток нет, супер-не-жизненная задача, но надо с чего-то начать). Нужно разложить картинки по трём папкам: \u003Ccode\u003E\u002Fcats\u003C\u002Fcode\u003E, \u003Ccode\u003E\u002Fdogs\u003C\u002Fcode\u003E и \u003Cs\u003E\u003Ccode\u003E\u002Fleather_bags\u003C\u002Fcode\u003E\u003C\u002Fs\u003E \u003Ccode\u003E\u002Fhumans\u003C\u002Fcode\u003E, поместив в каждую папку только фото с соответствующими объектами.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EЧто такое картинка\u002Fфотография?\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F074\u002Fe15\u002Ff04\u002F074e15f04c8347ab32f98ba04aeceb6c.png\"\u002F\u003E\u003Cbr\u002F\u003E\r\nПрактически везде в зрении принято работать с картинками в RGB-формате. У каждой картинки есть высота (H), ширина (W) и глубина, которая равна 3 (цвета). Таким образом, одну картинку можно представить как тензор размерности HxWx3 (каждый пиксель — это набор из трёх чисел — значений интенсивности в каналах). \u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F26c\u002F167\u002Fe3f\u002F26c167e3feb823e778b32278358053f9.jpg\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F26c\u002F167\u002Fe3f\u002F26c167e3feb823e778b32278358053f9.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПредставим, что с компьютерным зрением мы пока не знакомы, но знаем machine learning. Изображения — это просто числовые тензоры в памяти компьютера. Формализуем задачу в терминах машинного обучения: объекты — это картинки, их признаки — это значения в пикселях, ответ для каждого из объектов — метка класса (кошка, собака или человек). Это в чистом виде \u003Ca href=\"http:\u002F\u002Fwww.machinelearning.ru\u002Fwiki\u002Findex.php?title=%D0%9A%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F\"\u003Eзадача классификации\u003C\u002Fa\u003E. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EЕсли сейчас уже стало сложно..\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E… то лучше сначала прочитать первые 4 статьи из\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompany\u002Fods\u002Fblog\u002F322534\u002F\"\u003E Открытого курса OpenDataScience по ML\u003C\u002Fa\u003E и ознакомиться с более вводной статьёй по зрению, например, \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompany\u002Fyandex\u002Fblog\u002F203136\u002F\"\u003Eхорошая лекция в Малом ШАДе\u003C\u002Fa\u003E. \u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nМожно взять какие-нибудь методы из “классического” зрения или “классического” машинного обучения, то есть не нейросети. В основном эти методы заключаются в выделении на изображениях неких особенностей (особых точек) или локальных регионов, которые будут характеризовать картинку (“\u003Ca href=\"https:\u002F\u002Ftowardsdatascience.com\u002Fbag-of-visual-words-in-a-nutshell-9ceea97ce0fb\"\u003Eмешок визуальных слов\u003C\u002Fa\u003E”). Обычно всё это сводится к чему-то типа \u003Ca href=\"http:\u002F\u002Fwww.machinelearning.ru\u002Fwiki\u002Findex.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2\"\u003ESVM \u003C\u002Fa\u003Eнад \u003Ca href=\"https:\u002F\u002Fru.wikipedia.org\u002Fwiki\u002F%D0%93%D0%B8%D1%81%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D0%BD%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D1%8B%D1%85_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%B2\"\u003EHOG\u003C\u002Fa\u003E\u002F\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F106302\u002F\"\u003ESIFT\u003C\u002Fa\u003E. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nНо мы здесь собрались, чтобы поговорить о нейросетях, поэтому не хотим использовать придуманные нами признаки, а хотим, чтобы сеть сделала всё за нас. Наш классификатор будет принимать на вход признаки объекта и возвращать предсказание (метку класса). Здесь в качестве признаков выступают значения интенсивности в пикселях (см. модель картинки в под \u003Cbr\u002F\u003E\r\nспойлером выше). Помним, что картинка — это тензор размера (Height, Width, 3) (если она цветная). Сетке при обучении на вход всё это обычно подаётся не по одной картинке и не целым датасетом, а батчами, т.е. небольшими порциями объектов (например, 64 картинки в батче).\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nТаким образом, сеть принимает на вход тензор размера (BATCH_SIZE, H, W, 3). Можно “развернуть” каждую картинку в вектор-строку из H*W*3 чисел и работать со значениями в пикселях прямо как с признаками в машинном обучении, обычный \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMultilayer_perceptron\"\u003EMultilayer Perceptron (MLP)\u003C\u002Fa\u003E так и поступил бы, но это, честно говоря, такой себе бейзлайн, поскольку работа с пикселями как с вектор-строкой никак не учитывает, например, трансляционную инвариантность объектов на картинке. Тот же кот может быть как в середине фото, так и в углу, MLP эту закономерность не выучит.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЗначит нужно что-то поумнее, например, операция свёртки. И это уже про современное зрение, про \u003Cb\u003E\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F348000\u002F\"\u003Eсвёрточные нейронные сети\u003C\u002Fa\u003E\u003C\u002Fb\u003E:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EКод обучения свёрточной сети может выглядеть как-то так (на фреймворке PyTorch)\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Cpre\u003E\u003Ccode class=\"python\"\u003E# взято из официального туториала:\n# https:\u002F\u002Fpytorch.org\u002Ftutorials\u002Fbeginner\u002Fblitz\u002Fcifar10_tutorial.html\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\nfor epoch in range(2):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss \u002F 2000))\n            running_loss = 0.0\n\nprint('Finished Training')\n\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nПоскольку сейчас речь об \u003Ca href=\"http:\u002F\u002Fwww.machinelearning.ru\u002Fwiki\u002Findex.php?title=%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81_%D1%83%D1%87%D0%B8%D1%82%D0%B5%D0%BB%D0%B5%D0%BC\"\u003Eобучении с учителем\u003C\u002Fa\u003E, для тренировки нейросети нам нужны несколько компонент:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cul\u003E\r\n\u003Cli\u003EДанные (уже есть)\u003C\u002Fli\u003E\r\n\u003Cli\u003EАрхитектура сети (самое интересное)\u003C\u002Fli\u003E\r\n\u003Cli\u003EФункция потерь, которая будет говорить, как нейросети учиться (здесь это будет \u003Ca href=\"https:\u002F\u002Fmedium.com\u002F@vijendra1125\u002Funderstanding-entropy-cross-entropy-and-softmax-3b79d9b23c8a\"\u003Eкросс-энтропия\u003C\u002Fa\u003E)\u003C\u002Fli\u003E\r\n\u003Cli\u003EМетод оптимизации (будет менять веса сети в нужную сторону)\u003C\u002Fli\u003E\r\n\u003Cli\u003EЗадать гиперпараметры архитектуры и оптимизатора (например, размер шага оптимизатора, количество нейронов в слоях, коэффициенты регуляризации)\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003Cbr\u002F\u003E\r\nВ коде именно это и реализовано, сама свёрточная нейросеть описана в классе Net().\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЕсли хочется не спеша и с начала узнать про свёртки и свёрточные сети, рекомендую \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=Xul1DS08hSA&amp;list=PL0Ks75aof3ThkitsZbUOEQg7Ybl5kB_s3&amp;index=16\"\u003Eлекцию в Deep Learning School (ФПМИ МФТИ) (на русском)\u003C\u002Fa\u003E на эту тему, и, конечно же, \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=vT1JzLTH4G4&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv\"\u003Eкурс Стэнфорда cs231n (на английском)\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EDeep Learning School -- что это?\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Ca href=\"https:\u002F\u002Fwww.dlschool.org\u002F\"\u003EDeep Learning School\u003C\u002Fa\u003E при \u003Ca href=\"https:\u002F\u002Fmipt.ru\u002Fscience\u002Flabs\u002Finnovation\u002F\"\u003EЛаборатории Инноватики ФПМИ МФТИ\u003C\u002Fa\u003E — это организация, которая активно занимается разработкой открытого русскоязычного курса по нейросетям. В статье я буду несколько раз ссылаться на эти \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=RviskFqwF3M&amp;list=PL0Ks75aof3ThkitsZbUOEQg7Ybl5kB_s3\"\u003Eвидеоуроки\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Ffb1\u002F3ca\u002Fd97\u002Ffb13cad97db640053bb2c53c12b0f4a7.jpg\" alt=\"image\" width=\"650\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Ffb1\u002F3ca\u002Fd97\u002Ffb13cad97db640053bb2c53c12b0f4a7.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nЕсли вкратце, то операция свёртки позволяет находить паттерны на изображениях с учётом их вариативности. Когда обучаем свёрточные нейросети (eng: Convolutional Neural Networks), мы, по сути, находим фильтры свёрток (веса нейронов), которые хорошо описывают изображения, причём столь хорошо, чтобы можно было точно определить по ним класс. Способов построить такую сеть придумали много. Больше, чем вы думаете…\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ca name=\"3\"\u003E\u003C\u002Fa\u003E\u003Ch3\u003E Архитектуры свёрточных нейросетей: 1000 способов достичь одной цели\u003C\u002Fh3\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fc28\u002Fab9\u002F3c6\u002Fc28ab93c670c1e44258dc86064bb3a0c.png\" data-width=\"500\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nДа-да, \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fcompany\u002Fnixsolutions\u002Fblog\u002F430524\u002F\"\u003Eещё один обзор архитектур\u003C\u002Fa\u003E. Но здесь я постараюсь сделать его максимально актуальным!\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nСначала была \u003Ca href=\"http:\u002F\u002Fyann.lecun.com\u002Fexdb\u002Fpublis\u002Fpdf\u002Flecun-98.pdf\"\u003ELeNet\u003C\u002Fa\u003E, она помогла Яну ЛеКуну распознавать цифры в 1998 году. Это была первая свёрточная нейросеть для классификации. Её основная фишка была в том, что она в принципе стала использовать \u003Ca href=\"https:\u002F\u002Fmedium.freecodecamp.org\u002Fan-intuitive-guide-to-convolutional-neural-networks-260c2de0a050\"\u003Econvolution и pooling\u003C\u002Fa\u003E операции.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fbd4\u002F27e\u002F5e2\u002Fbd427e5e2943ebf58409e42538c4e131.png\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nДалее было некоторое затишье в развитии сеток, однако «железо» не стояло на месте, развивались эффективные вычисления на GPU и \u003Cabbr title=\"Accelerated Linear Algebra\"\u003EXLA\u003C\u002Fabbr\u003E. В 2012 году появилась AlexNet, она выстрелила в соревновании ILSVRC (\u003Ca href=\"http:\u002F\u002Fwww.image-net.org\u002Fchallenges\u002FLSVRC\u002F\"\u003EImageNet Large-Scale Visual Recognition Challenge\u003C\u002Fa\u003E). \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EНебольшое отступление про ILSVRC\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EК 2012 году был дан собран \u003Ca href=\"http:\u002F\u002Fwww.image-net.org\"\u003EImageNet\u003C\u002Fa\u003E, и для соревнования ILSVRC использовалась его подвыборка из тысяч картинок и 1000 классов. Сейчас в ImageNet ~14 миллионов картинок и 21841 класс (взято с оф. сайта), но для соревнования всё равно обычно выделяют лишь подмножество. ILSVRC тогда стал самым крупным ежегодным соревнованием по классификации изображений. Кстати, недавно придумали, как можно \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1709.05011.pdf\"\u003Eобучить на ImageNet'е за считанные минуты\u003C\u002Fa\u003E. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nИменно на ImageNet (в ILSVRC) с 2010 по 2018 получали \u003Cabbr title=\"State-of-the-Art\"\u003ESOTA\u003C\u002Fabbr\u003E-нейросети в задаче классификации изображений. Правда уже с 2016 года более актуальны соревнования по локализации, детектирования и понимания сцены, а не классификацию.\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nОбычно различные \u003Ca href=\"https:\u002F\u002Fmedium.com\u002F@RaghavPrabhu\u002Fcnn-architectures-lenet-alexnet-vgg-googlenet-and-resnet-7c81c017b848\"\u003Eобзоры архитектур\u003C\u002Fa\u003E проливают свет на те, что были первыми на ILSVRC с 2010 до 2016 года, и на некоторые отдельные сети. Чтобы не загромождать рассказ, я поместил их под спойлер ниже, постаравшись подчеркнуть основные идеи:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EАрхитектуры с 2012 по 2015 год\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Cdiv class=\"scrollable-table\"\u003E\u003Ctable\u003E\r\n\u003Ctr\u003E\r\n\u003Cth\u003EГод\u003C\u002Fth\u003E\r\n\u003Cth\u003EСтатья\u003C\u002Fth\u003E\r\n\u003Cth\u003EКлючевая идея\u003C\u002Fth\u003E\r\n\u003Cth\u003EВес\u003C\u002Fth\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2012\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fpapers.nips.cc\u002Fpaper\u002F4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\"\u003EAlexNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003Eиспользовать две свёртки подряд; делить обучение сети на две параллельных ветки\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E240 MB\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2013\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1311.2901v3.pdf\"\u003EZFNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003Eразмер фильтров, число фильтров в слоях\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E--\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2013\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1312.6229\"\u003EOverfeat\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003Eодин из первых нейросетевых детекторов\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E--\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2014\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1409.1556\"\u003EVGG\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003Eглубина сети (13-19 слоёв), использование нескольких блоков Conv-Conv-Pool с меньшим размером свёрток (3х3)\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E549MB (VGG-19)\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2014\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1409.4842\"\u003EInception (v1) (она же GoogLeNet)\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E1х1-свёртка (идея из \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1312.4400.pdf\"\u003ENetwork-in-Network\u003C\u002Fa\u003E), auxilary losses (или \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1505.02496\"\u003Edeep supervision\u003C\u002Fa\u003E), стекинг выходов нескольких свёрток (Inception-блок)\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E--\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2015\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1512.03385\"\u003EResNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Ftowardsdatascience.com\u002Fan-overview-of-resnet-and-its-variants-5281e2f56035\"\u003Eresidual connections\u003C\u002Fa\u003E, очень большая глубина (152 слоя..)\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E98 MB (ResNet-50), 232 MB (ResNet-152)\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fz3\u002Fi4\u002Fb4\u002Fz3i4b4pxfnulxzfszysn_usqn_c.png\" data-width=\"500\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nИдеи всех этих архитектур (кроме ZFNet, её обычно мало упоминают) в своё время были новым словом в нейросетях для зрения. Однако и после 2015 было ещё очень много важных улучшений, например, Inception-ResNet, Xception, DenseNet, SENet. Ниже я постарался собрать их в одном месте.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EАрхитектуры с 2015 по 2019 год\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Cdiv class=\"scrollable-table\"\u003E\u003Ctable\u003E\r\n\u003Ctr\u003E\r\n\u003Cth\u003EГод\u003C\u002Fth\u003E\r\n\u003Cth\u003EСтатья\u003C\u002Fth\u003E\r\n\u003Cth\u003EКлючевая идея\u003C\u002Fth\u003E\r\n\u003Cth\u003EВес\u003C\u002Fth\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2015\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1512.00567\"\u003EInception v2 и v3\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F302242\u002F\"\u003Eразложение свёрток в свёртки 1хN и Nx1\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E92 MB\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2016\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1602.07261\"\u003EInception v4 и Inception-ResNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F303196\u002F\"\u003Eсовмещение Inception и ResNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E215 MB\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2016-17\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1611.05431\"\u003EResNeXt\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E2 место ILSVRC, использование многих веток ( “обобщённый” Inception-блок)\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E--\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2017\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1610.02357\"\u003EXception\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F347564\u002F\"\u003Edepthwise separable convolution\u003C\u002Fa\u003E, меньше весит при сравнимой с Inception точности\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E88 MB\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2017\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1608.06993\"\u003EDenseNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Ftowardsdatascience.com\u002Fdensenet-2810936aeebb\"\u003EDense-блок\u003C\u002Fa\u003E; лёгкая, но точная\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E33 MB (DenseNet-121), 80 MB (DenseNet-201)\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2018\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"http:\u002F\u002Fopenaccess.thecvf.com\u002Fcontent_cvpr_2018\u002Fpapers\u002FHu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf\"\u003ESENet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fshaoanlu.wordpress.com\u002F2017\u002F08\u002F17\u002Fsenet-winner-of-imagenet-2017\u002F\"\u003ESqueeze-and-Excitation блок\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fhujie-frank\u002FSENet\"\u003E46 MB (SENet-Inception), 440 MB (SENet-154)\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nБольшинство из этих моделей для PyTorch можно найти \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FCadene\u002Fpretrained-models.pytorch\"\u003Eздесь\u003C\u002Fa\u003E, а ещё есть вот \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Faaron-xichen\u002Fpytorch-playground\"\u003Eтакая классная штука\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВы могли заметить, что всё это дело весит довольно много (хотелось бы 20 MB максимум, а то поменьше), в то время как сейчас повсеместно используют мобильные устройства и приобретает популярность \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F259243\u002F\"\u003EIoT\u003C\u002Fa\u003E, а значит сетки хочется использовать и там.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EСвязь веса модели и скорости работы\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EПоскольку нейросети внутри себя лишь перемножают тензоры, то количество операций умножения (читай: количество весов) напрямую влияет на скорость работы (если не используются трудоёмкие пост- или предобработка). Сама скорость работы сети зависит от реализации (фреймворка), железа, на котором выполняется, и от размера входной картинки.\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nАвторы многих статей пошли по пути изобретения быстрых архитектур, я собрал их методы под спойлером ниже:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EЛегковесные архитектуры CNN\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Cdiv class=\"scrollable-table\"\u003E\u003Ctable\u003E\r\n\u003Ctr\u003E\r\n\u003Cth\u003EГод\u003C\u002Fth\u003E\r\n\u003Cth\u003EСтатья\u003C\u002Fth\u003E\r\n\u003Cth\u003EКлючевая идея\u003C\u002Fth\u003E\r\n\u003Cth\u003EВес\u003C\u002Fth\u003E\r\n\u003Cth\u003EПример реализации\u003C\u002Fth\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2016\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1602.07360\"\u003ESqueezeNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"http:\u002F\u002Feverything.explained.today\u002FSqueezeNet\u002F\"\u003EFireModule сжатия-разжатия\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E0.5 MB\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FDeepScale\u002FSqueezeNet\"\u003ECaffe\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2017\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1707.07012\"\u003ENASNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"http:\u002F\u002Feverything.explained.today\u002FSqueezeNet\u002F\"\u003E\u003C\u002Fa\u003E\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F408641\u002F\"\u003Eполучена нейронным поиском архитектур, это сеть из разряда AutoML\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E23 MB\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FCadene\u002Fpretrained-models.pytorch\u002Fblob\u002Fmaster\u002Fpretrainedmodels\u002Fmodels\u002Fnasnet.py\"\u003EPyTorch\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2017\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1707.01083\"\u003EShuffleNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fmedium.com\u002Fsyncedreview\u002Fshufflenet-an-extremely-efficient-convolutional-neural-network-for-mobile-devices-72c6f5b01651\"\u003Epointwise group conv, channel shuffle\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E--\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ffarmingyard\u002FShuffleNet\"\u003ECaffe\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2017\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1704.04861\"\u003EMobileNet (v1)\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F352804\u002F\"\u003Edepthwise separable convolutions и много других трюков\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E16 MB\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FZehaos\u002FMobileNet\"\u003ETensorFlow\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2018\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1801.04381\"\u003EMobileNet (v2)\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F352804\u002F\"\u003Eрекомендую эту статью на Хабре\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E14 MB\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fshicai\u002FMobileNet-Caffe\"\u003ECaffe\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2018\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1803.10615\"\u003ESqueezeNext\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003Eсм. картиночки в оригинальном репозитории\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E--\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Famirgholami\u002FSqueezeNext\"\u003ECaffe\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2018\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1803.10615\"\u003EMnasNet\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fai.googleblog.com\u002F2018\u002F08\u002Fmnasnet-towards-automating-design-of.html\"\u003Eнейропоиск архитектуры специально под мобильные устройства с помощью RL\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E~2 MB\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftensorflow\u002Ftpu\u002Ftree\u002Fmaster\u002Fmodels\u002Fofficial\u002Fmnasnet\"\u003ETensorFlow\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003Ctr\u003E\r\n\u003Ctd\u003E2019\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1905.02244\"\u003EMobileNet (v3)\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003Ctd\u003Eона вышла, пока я писал статью :)\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E--\u003C\u002Ftd\u003E\r\n\u003Ctd\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FRandl\u002FMobileNetV3-pytorch\"\u003EPyTorch\u003C\u002Fa\u003E\u003C\u002Ftd\u003E\r\n\u003C\u002Ftr\u003E\r\n\u003C\u002Ftable\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nЦифры во всех таблицах \u003Cs\u003Eвзяты с потолка\u003C\u002Fs\u003E из репозиториев, из \u003Ca href=\"https:\u002F\u002Fkeras.io\u002Fapplications\u002F\"\u003Eтаблицы Keras Applications\u003C\u002Fa\u003E и из \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1605.07678\"\u003Eэтой статьи\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВы спросите: “Для чего ты написал про весь этот “зоопарк” моделей? И почему всё же задача классификации? Мы же хотим научить машины видеть, а классификация — лишь какая-то узкая задача..”. Дело в том, что нейросети для детектирования объектов, оценки позы\u002Fточек, ре-идентификации и поиска по картинке используют в качестве \u003Cb\u003E\u003Cabbr title=\"основа, дословно -- позвоночник\"\u003Ebackbone\u003C\u002Fabbr\u003E\u003C\u002Fb\u003E именно модели для классификации, и уже от них зависит 80% успеха. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nНо хочется как-то больше доверять CNN, а то напридумывали чёрных коробок, а что «внутри» — не очевидно. Чтобы лучше понимать механизм функционирования свёрточных сетей, исследователи придумали использовать визуализацию.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ca name=\"4\"\u003E\u003C\u002Fa\u003E\u003Ch3\u003EВизуализация свёрточных нейросетей: покажи мне страсть\u003C\u002Fh3\u003E\u003Cbr\u002F\u003E\r\nВажным шагом к осознанию того, что происходит внутри свёрточных сетей, стала статья \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1311.2901\"\u003E«Visualizing and Understanding Convolutional Networks»\u003C\u002Fa\u003E. В ней авторы предложили несколько способов визуализации того, на что именно (на какие части картинки) реагируют нейроны в разных слоях CNN (рекомендую также посмотреть \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=6wcs6szJWMY&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=12\"\u003Eлекцию Стэнфорда на эту тему\u003C\u002Fa\u003E). Результаты получились весьма впечатляющие: авторы показали, что первые слои свёрточной сети реагируют на какие-то «низкоуровневые вещи» по типу краёв\u002Fуглов\u002Fлиний, а последние слои реагируют уже на целые части изображений (см. картинку ниже), то есть уже несут в себе некоторую семантику.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Ffz\u002Fvm\u002Fym\u002Ffzvmymab57wgircssyfgxiaomvy.jpeg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Ffz\u002Fvm\u002Fym\u002Ffzvmymab57wgircssyfgxiaomvy.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nДалее \u003Ca href=\"http:\u002F\u002Fyosinski.com\u002Fdeepvis\"\u003Eпроект по глубокой визуализации от Cornell University и компании\u003C\u002Fa\u003E продвинул визуализацию ещё дальше, в то время как \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F367643\u002F\"\u003Eзнаменитый DeepDream\u003C\u002Fa\u003E научился искажать в \u003Cs\u003Eнаркоманском\u003C\u002Fs\u003E интересном стиле (ниже картинка с \u003Ca href=\"https:\u002F\u002Fdeepdreamgenerator.com\u002F#gallery\"\u003Edeepdreamgenerator.com\u003C\u002Fa\u003E). \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fe55\u002F809\u002F63f\u002Fe5580963fdfb998bfe2103f4cbf5aa8c.jpg\" alt=\"image\" width=\"500\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Fe55\u002F809\u002F63f\u002Fe5580963fdfb998bfe2103f4cbf5aa8c.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВ 2017 году вышла \u003Ca href=\"https:\u002F\u002Fdistill.pub\u002F2017\u002Ffeature-visualization\u002F\"\u003Eочень хорошая статья на Distill\u003C\u002Fa\u003E, в которой они провели подробный анализ того, что «видит» каждый из слоёв, и совсем недавно (в марте 2019) Google изобрела \u003Ca href=\"https:\u002F\u002Fdistill.pub\u002F2019\u002Factivation-atlas\u002F\"\u003Eактивационные атласы\u003C\u002Fa\u003E: своеобразные карты, которые можно строить для каждого слоя сети, что приближает к понимаю общей картины работы CNN.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fb-\u002F-k\u002Fiw\u002Fb--kiw7-vibdk8vpuzfxhbagkuu.png\" data-width=\"700\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЕсли хочется самому поиграться с визуализацией, я бы рекомендовал \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftensorflow\u002Flucid\"\u003ELucid \u003C\u002Fa\u003E и \u003Ca href=\"https:\u002F\u002Ftensorspace.org\u002Fhtml\u002Fplayground\u002Fmobilenetv1.html\"\u003ETensorSpace\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nОкей, кажется, CNN и правда в некоторой степени можно верить. Нужно научиться использовать это и в других задачах, а не только в классификации. В этом нам помогут извлечение Embedding'ов картинок и Transfer Learning.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ca name=\"5\"\u003E\u003C\u002Fa\u003E\u003Ch2\u003EЯ и сам своего рода хирург: извлекаем фичи из нейросетей\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\nПредставим, что есть картинка, и мы хотим найти похожие на неё визуально (так умеет, например, поиск по картинке в Яндекс.Картинки). Раньше (до нейросетей) инженеры для этого извлекали фичи вручную, например, придумывая что-то, что хорошо описывает картинку и позволит её сравнивать с другими. В основном, эти методы (\u003Ca href=\"https:\u002F\u002Fru.wikipedia.org\u002Fwiki\u002F%D0%93%D0%B8%D1%81%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0_%D0%BD%D0%B0%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D1%8B%D1%85_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%B2\"\u003EHOG\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F106302\u002F\"\u003ESIFT\u003C\u002Fa\u003E) оперируют \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FImage_gradient\"\u003Eградиентами картинок\u003C\u002Fa\u003E, обычно именно эти штуки и называют «классическими» дескрипторами изображений. Особо интересующихся отсылаю к \u003Ca href=\"https:\u002F\u002Fmedium.com\u002Fmachine-learning-world\u002Ffeature-extraction-and-similar-image-search-with-opencv-for-newbies-3c59796bf774\"\u003Eстатье\u003C\u002Fa\u003E и к \u003Ca href=\"https:\u002F\u002Fwww.lektorium.tv\u002Fcourse\u002F22847\"\u003Eкурсу Антона Конушина\u003C\u002Fa\u003E (это не реклама, просто курс хороший :)\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F5cd\u002Fb10\u002Fea8\u002F5cdb10ea8f19fe29432265e906640a90.jpg\" alt=\"image\" width=\"500\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F5cd\u002Fb10\u002Fea8\u002F5cdb10ea8f19fe29432265e906640a90.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nИспользуя нейросети, мы можем не придумывать самому эти фичи и эвристики, а правильно обучить модель и потом \u003Cb\u003Eвзять за признаки картинки выход одного или нескольких слоёв сети\u003C\u002Fb\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F887\u002Fd76\u002Feb4\u002F887d76eb431bcaf434ff70e2e0f2d4b0.png\" data-width=\"650\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nПосмотрев на все архитектуры выше поближе, становится понятно, что в CNN для классификации есть два этапа:\u003Cbr\u002F\u003E\r\n1). \u003Cb\u003EFeature extractor\u003C\u002Fb\u003E слои для выделения информативных фич из картинок с помощью свёрточных слоёв\u003Cbr\u002F\u003E\r\n2). Обучение поверх этих фич \u003Cb\u003EFully Connected (FC)\u003C\u002Fb\u003E слоёв-классификаторов\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F55d\u002Fca5\u002F358\u002F55dca535836121c65546bc11e2d457c1.png\" data-width=\"500\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003EEmbedding'и картинок (фичи)\u003C\u002Fb\u003E — это как раз про то, что можно брать в качестве информативного описания картинок их признаки после Feature extractor’а свёрточной нейросети (правда их можно по-разному агрегировать). То есть обучили сеть на классификацию, а потом просто берём выход перед классификационными слоями. Эти признаки называют \u003Ci\u003Eфичами\u003C\u002Fi\u003E, \u003Ci\u003Eнейросетевыми дескрипторами\u003C\u002Fi\u003Eили \u003Ci\u003Eэмбеддингами\u003C\u002Fi\u003E картинки (правда обычно эмбеддинги принято в NLP, так как это зрение, я чаще буду говорить \u003Ci\u003Eфичи\u003C\u002Fi\u003E). Обычно это какой-то числовой вектор, например, 128 чисел, с которым уже можно работать. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EА как же автоэнкодеры?\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EДа, на самом деле фичи можно получить и \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F331382\u002F\"\u003Eавтоэнкодерами\u003C\u002Fa\u003E. На моей практике делали по-разному, но, например, в статьях по ре-идентификации (о которой речь будет дальше), чаще всё же берут фичи после extractor'a, а не обучают для этого автоэнкодер. Мне кажется, стоит провести эксперименты в обоих направлениях, если стоит вопрос о том, что работает лучше.\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nТаким образом, пайплайн решения \u003Cb\u003Eзадачи поиска по картинке\u003C\u002Fb\u003E может быть устроен просто: прогоняем картинки через CNN, берём признаки с нужных слоёв и сравниваем эти фичи друг с другом у разных картинок. Например, банально считаем Евклидово расстояние этих векторов.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"http:\u002F\u002Fapi.ning.com\u002Ffiles\u002F1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg\u002Ftransferlearning.png\" data-width=\"500\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003ETransfer Learning\u003C\u002Fb\u003E — широко известная техника эффективного дообучения нейросетей, которые уже обучены на каком-то определённом датасете, под свою задачу. Часто ещё говорят Fine Tuning вместо Transfer Learning, в \u003Ca href=\"http:\u002F\u002Fcs231n.github.io\u002Ftransfer-learning\u002F\"\u003Eконспектах Стэнфордского курса cs231n\u003C\u002Fa\u003E эти понятия разделяют, мол, Transfer Learning — это общая идея, а Fine Tuning — одна из реализаций техники. Нам это в дальнейшем не так важно, главное понимать, что мы просто можем дообучить сеть хорошо предсказывать на новом датасете, стартуя не со случайных весов, а с обученных на каком-нибудь большом по типу ImageNet. Это особенно актуально, когда данных мало, а задачу хочется решить качественно. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EПодробнее про Transfer Learning\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1411.1792\"\u003EОригинальная статья\u003C\u002Fa\u003E, а вообще \u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=U12tq9l9xy8\"\u003Eзачем читать много текста, если можно посмотреть видео\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nОднако просто брать нужные фичи и делать дообучение с датасета на датасет может быть недостаточно, например, для задач поиска похожих лиц\u002Fлюдей\u002Fчего-то специфичного. Фотографии одного и того же человека визуально иногда могут быть даже более непохожи, чем фотографии разных людей. Нужно заставить сеть выделять именно те признаки, которые присущи одному человеку\u002Fобъекту, даже если нам это сделать глазами сложно. Добро пожаловать в мир \u003Cb\u003Erepresentation learning\u003C\u002Fb\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ca name=\"6\"\u003E\u003C\u002Fa\u003E\u003Ch2\u003EДержись рядом: representation learning для людей и лиц\u003C\u002Fh2\u003E\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EПримечание по терминологии\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EЕсли почитать научные статьи, то иногда складывается впечатление, что некоторые авторы понимают словосочетание \u003Cb\u003Emetric learning\u003C\u002Fb\u003E по-разному, и нет какого-то единого мнения на счёт того, какие методы называть metric learning, а какие нет. Именно поэтому в данной статье я решил избежать именно этого словосочетания и использовал более логичное \u003Cb\u003Erepresentation learning\u003C\u002Fb\u003E, некоторые читатели могут с этим не согласиться — буду рад обсудить в комментариях.\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nПоставим задачи:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cul\u003E\r\n\u003Cli\u003E\u003Cb\u003EЗадача 1\u003C\u002Fb\u003E: есть галерея (набор) фотографий лиц людей, хотим, чтобы по новому фото сеть умела отвечать либо именем человека из галереи (мол, это он), либо говорила, что такого человека в галерее нет (и, возможно, добавляем в неё нового человека)\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Ffbc\u002F3ad\u002Ff28\u002Ffbc3adf280e28f7bb71246f50c1e8d9e.jpg\" width=\"300\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002Ffbc\u002F3ad\u002Ff28\u002Ffbc3adf280e28f7bb71246f50c1e8d9e.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Cb\u003EЗадача 2\u003C\u002Fb\u003E: то же самое, но работаем не с фотографиями лиц, а с кропами людей в полный рост\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fjh\u002F43\u002Fxs\u002Fjh43xsjjgxixbw8cmo1idxage5a.jpeg\" width=\"400\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fjh\u002F43\u002Fxs\u002Fjh43xsjjgxixbw8cmo1idxage5a.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПервую задачу обычно называют \u003Cb\u003Eраспознаванием лиц\u003C\u002Fb\u003E, вторую — \u003Cb\u003Eре-идентификацией\u003C\u002Fb\u003E (сокращённо \u003Ci\u003EReid\u003C\u002Fi\u003E). Я объединил их в один блок, поскольку в их решениях сегодня используются схожие идеи: для того, чтобы выучивать эффективные эмбеддинги картинок, которые могут справляться и с довольно сложными ситуациями, сегодня используют различные типы лоссов, такие как, например, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1503.03832\"\u003Etriplet loss\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1704.01719.pdf\"\u003Equadruplet loss\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1707.07391.pdf\"\u003Econtrastive-center loss\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Felib.dlr.de\u002F116408\u002F1\u002FWACV2018.pdf\"\u003Ecosine loss\u003C\u002Fa\u003E. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002F9s\u002Fpj\u002Fcm\u002F9spjcm6xbc2j2ip_wgri9wutjpi.jpeg\" width=\"550\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F9s\u002Fpj\u002Fcm\u002F9spjcm6xbc2j2ip_wgri9wutjpi.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nЕщё есть прекрасные \u003Ca href=\"https:\u002F\u002Fhackernoon.com\u002Fone-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e\"\u003Eсиамские\u003C\u002Fa\u003E \u003Ca href=\"https:\u002F\u002Fsorenbouma.github.io\u002Fblog\u002Foneshot\u002F\"\u003Eсети\u003C\u002Fa\u003E, однако их я, честно, сам не использовал. Кстати, “решает” не только сам лосс, а то, как для него семплировать пары positive’ов и negative’ов, это подчёркивают авторы статьи \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1706.07567\"\u003ESampling matters in deep embedding learning\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nСуть всех этих лоссов и сиамских сетей проста — хотим, чтобы картинки одного класса (человека) в латентном пространстве фич (эмбеддингов) были “близко”, а разных классов (людей) — “далеко”. Близость обычно меряется так: берутся эмбеддинги картинок из нейросети (например, вектор из 128 чисел) и либо считаем обычное \u003Ca href=\"https:\u002F\u002Fru.wikipedia.org\u002Fwiki\u002F%D0%95%D0%B2%D0%BA%D0%BB%D0%B8%D0%B4%D0%BE%D0%B2%D0%B0_%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D0%BA%D0%B0\"\u003EЕвклидово расстояние\u003C\u002Fa\u003E между этими векторами, либо \u003Ca href=\"https:\u002F\u002Fru.wikipedia.org\u002Fwiki\u002F%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C\"\u003Eкосинусную близость.\u003C\u002Fa\u003E Как именно мерить — лучше подбирать на своём датасете\u002Fзадаче.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nСхематично пайплайн решения задач на representation learning выглядит примерно так:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002F16\u002Fuh\u002Fn8\u002F16uhn8l_iahuy-bcv_e4vohx1je.png\" data-width=\"850\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EНо если быть более точным, то вот так\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Cb\u003EНа стадии обучения\u003C\u002Fb\u003E: обучаем нейросеть либо на классификацию (Softmax + CrossEntropy), либо с помощью специального лосса (Triplet, Contrastive, etc.). Во втором случае ещё нужно правильно подбирать positive'ы и negative'ы в каждом батче\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cb\u003EНа стадии предсказания\u003C\u002Fb\u003E: если это был именно какой-то особый лосс по типу триплета, то он на вход принимал уже эмбеддинги — их и берём. Если была классификация, то тут нужно экспериментировать — можно брать фичи с какого-то из свёрточных слоёв, а можно и вероятности после классификатора (да, так делают и это \u003Ci\u003Eработает\u003C\u002Fi\u003E). Далее ищем расстояние от пришедшей в тесте картинки до всех картинок из галереи и выдаём метку ближайшей. Расстояние меряем \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCosine_similarity\"\u003Eкосинусом\u003C\u002Fa\u003E или \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FEuclidean_distance\"\u003EЕвклидовой метрикой\u003C\u002Fa\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nКонкретно по \u003Cb\u003Eраспознаванию лиц\u003C\u002Fb\u003E есть несколько хороших статей: \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1804.06655\"\u003Eстатья-обзор (\u003Cb\u003EMUST READ!\u003C\u002Fb\u003E)\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1503.03832\"\u003EFaceNet\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1801.07698.pdf\"\u003EArcFace\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1801.09414.pdf\"\u003ECosFace\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" alt=\"image\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F208\u002F1b9\u002Fd34\u002F2081b9d346f74503302b8fd2c7265ef5.png\" data-width=\"700\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nРеализаций тоже немало: \u003Ca href=\"http:\u002F\u002Fblog.dlib.net\u002F2017\u002F02\u002Fhigh-quality-face-recognition-with-deep.html\"\u003Edlib\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fcmusatyalab.github.io\u002Fopenface\u002F\"\u003EOpenFace\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fdavidsandberg\u002Ffacenet\"\u003EFaceNet repo\u003C\u002Fa\u003E, да и \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F317798\u002F\"\u003Eна Хабре про это уже давно было рассказано\u003C\u002Fa\u003E. Кажется, за последнее время добавились только ArcFace и CosFace (пишите в комментарии, если я здесь что-то упустил, буду рад узнать что-то ещё).\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nОднако сейчас больше мода не на распознавание лиц, а на их \u003Ca href=\"https:\u002F\u002Fthispersondoesnotexist.com\u002F\"\u003Eгенерацию\u003C\u002Fa\u003E, не так ли?\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Ft9\u002Fk3\u002Fzv\u002Ft9k3zvmuf30yzmcvlube_okh5ey.png\" data-width=\"500\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\nВ свою очередь, в задаче \u003Cb\u003Eре-идентификации\u003C\u002Fb\u003E сейчас бурная активность, статьи выходят каждый месяц, люди пробуют разные подходы, что-то работает уже сейчас, что-то пока ещё не очень. \u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Ff_\u002Fpe\u002Fcd\u002Ff_pecd2dvv5kbatdpj0nkk4aapm.png\"\u002F\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nПоясню суть задачи Reid на примере: есть галерея с \u003Cabbr title=\"вырезанная из большой фотографии детекция человка\"\u003Eкропами\u003C\u002Fabbr\u003E людей, например, 10 людей, у каждого по 5 кропов (могут быть с разных сторон), то есть 50 фотографий в галерее. Приходит новая детекция (кроп), и надо сказать, какой это человек из галереи или сказать, что его там нет и завести для него новый ID. Задача усложняется тем, что детекции человека приходят с разных ракурсов: спереди, сзади, сбоку, \u003Cs\u003Eснизу\u003C\u002Fs\u003E, и плюс камеры, с которых фото приходят, тоже разные (разные освещения\u002Fбалансы белого и т.д.).\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"\u002Fimg\u002Fimage-loader.svg\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fca\u002Fpn\u002Fgr\u002Fcapngrtiskbeltdx0oq_wfntw0i.png\" data-width=\"700\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nК слову, в нашей \u003Ca href=\"http:\u002F\u002Fneuruslab.ru\u002F\"\u003Eлаборатории\u003C\u002Fa\u003E Reid — одна из ключевых задач. Статей выходит действительно немало, какие-то из них про новый более эффективный лосс, какие-то только про новый способ добычи negative'ов и positive'ов.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nХороший обзор старых методов по Reid есть в \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1610.02984\"\u003Eстатье 2016 года\u003C\u002Fa\u003E. Сейчас, как я уже писал выше, применяются два подхода — классификация или representation learning. Однако есть специфика задачи, с ней исследователи борются по-разному, например, авторы \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fhuanghoujing\u002FAlignedReID-Re-Production-Pytorch\"\u003EAligned Re-Id\u003C\u002Fa\u003E предложили специальным образом выравнивать фичи (да, они смогли улучшить сеть с помощью динамического программирования\u003Cs\u003E, Карл\u003C\u002Fs\u003E), в \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1711.10295.pdf\"\u003Eдругой статье\u003C\u002Fa\u003E предложили применить \u003Ca href=\"https:\u002F\u002Fyoutu.be\u002F2t05gq13xy0\"\u003EGenerative Adversarial Networks (GAN)\u003C\u002Fa\u003E.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EЕсть ещё несколько трюков\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003E\u003Cul\u003E\r\n\u003Cli\u003EДобавить новый тип аугментации, \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1708.04896.pdf\"\u003Eзакрывая случайные участки картинки\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1604.07807v2.pdf\"\u003EСлить вместе handcrafted-фичи и фичи из сетки\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1803.11333\"\u003EУчесть разницу в ракурсах камер явно при обучении\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1611.05244.pdf\"\u003EПравильно делать Transfer Learning с одного датасета на другой\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F456\u002F66e\u002F9c0\u002F45666e9c0608373c31452aeb6a197477.jpg\" alt=\"image\" width=\"650\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fpost_images\u002F456\u002F66e\u002F9c0\u002F45666e9c0608373c31452aeb6a197477.jpg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EДемотиватор\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EНесмотря на все эти продвинутые методы, в моих экспериментах, почему-то, лучше всего себя показал именно подход с классификацией. Возможно, я что-то не учёл, но пока что немного грустно, что придумали столько всего, а в итоге работает \u003Cs\u003Eстарая добрая логистическая регрессия\u003C\u002Fs\u003E классификация. Но главное — пробовать и не сдаваться!\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nИз реализаций хочется обязательно упомянуть \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FCysu\u002Fopen-reid\"\u003EOpenReid\u003C\u002Fa\u003E и \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FKaiyangZhou\u002Fdeep-person-reid\"\u003ETorchReid\u003C\u002Fa\u003E. Обратите внимание на сам код — на мой взгляд, он написан грамотно с точки зрения архитектуры фреймворка, подробнее \u003Ca href=\"https:\u002F\u002Fcysu.github.io\u002Fopen-reid\u002Fnotes\u002Foverview.html\"\u003Eздесь\u003C\u002Fa\u003E. Плюс они оба на PyTorch, и в Readme есть много ссылок на статьи по Person Re-identification, это приятно.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nВообще особый спрос на face- и reid-алгоритмы сейчас в Китае (\u003Ca href=\"https:\u002F\u002Fmeduza.io\u002Ffeature\u002F2018\u002F02\u002F11\u002Fv-kitae-sozdayut-totalnuyu-sistemu-raspoznavaniya-lits-grazhdan-ona-pomozhet-lovit-prestupnikov-i-sobirat-dannye-na-vseh-ostalnyh\"\u003Eесли вы понимаете, о чём я\u003C\u002Fa\u003E). Мы на очереди? Кто знает…\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ch3\u003EСлово про ускорение нейросетей\u003C\u002Fh3\u003E\u003Cbr\u002F\u003E\r\nМы уже говорили о том, что можно просто придумать легковесную архитектуру. Но как быть, если сеть уже обучена и она крута, а сжать её всё равно нужно? В таком случае может помочь один (или все) из следующих методов:\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cul\u003E\r\n\u003Cli\u003EДистилляция: \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1503.02531\"\u003Eраз\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fnervanasystems.github.io\u002Fdistiller\u002Fknowledge_distillation.html\"\u003Eдва\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fmedium.com\u002Fneural-machines\u002Fknowledge-distillation-dc241d7c2322\"\u003Eтри\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003EКвантизация: \u003Ca href=\"https:\u002F\u002Fheartbeat.fritz.ai\u002F8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd\"\u003Eраз\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fnervanasystems.github.io\u002Fdistiller\u002Fquantization.html\"\u003Eдва\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003Cli\u003EПрунинг: \u003Ca href=\"https:\u002F\u002Fhabr.com\u002Fru\u002Fpost\u002F413939\u002F\"\u003Eраз\u003C\u002Fa\u003E, \u003Ca href=\"https:\u002F\u002Fjacobgil.github.io\u002Fdeeplearning\u002Fpruning-deep-learning\"\u003Eдва\u003C\u002Fa\u003E\u003C\u002Fli\u003E\r\n\u003C\u002Ful\u003E\u003Cbr\u002F\u003E\r\nНу и правило использовать не float64, а, например, float32 никто не отменял. Есть даже свежая \u003Ca href=\"https:\u002F\u002Farxiv.org\u002Fabs\u002F1904.11943\"\u003Eстатья про low-precision training\u003C\u002Fa\u003E. Недавно, кстати, Google представил \u003Ca href=\"https:\u002F\u002Fai.googleblog.com\u002F2019\u002F04\u002Fmorphnet-towards-faster-and-smaller.html\"\u003EMorphNet\u003C\u002Fa\u003E, которая (вроде как) помогает автоматически сжимать модель.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Ca name=\"7\"\u003E\u003C\u002Fa\u003E\u003Ch3\u003EА что дальше?\u003C\u002Fh3\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv style=\"text-align:center;\"\u003E\u003Cimg src=\"https:\u002F\u002Fhabrastorage.org\u002Fr\u002Fw780q1\u002Fwebt\u002Fue\u002F22\u002Fe1\u002Fue22e11md3zjexlxq3jxsf-kx18.jpeg\" alt=\"image\" width=\"500\" data-src=\"https:\u002F\u002Fhabrastorage.org\u002Fwebt\u002Fue\u002F22\u002Fe1\u002Fue22e11md3zjexlxq3jxsf-kx18.jpeg\" data-blurred=\"true\"\u002F\u003E\u003C\u002Fdiv\u003E\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nМы обсудили действительно много полезных и прикладных вещей в DL и CV: классификация, архитектуры сетей, визуализация, эмбеддинги. Однако в современном зрении есть ещё и другие важные задачи: детектирование, сегментация, понимание сцены. Если речь про видео, то хочется объекты \u003Ca href=\"https:\u002F\u002Fru.wikipedia.org\u002Fwiki\u002F%D0%A2%D1%80%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3_(%D0%BA%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D0%BD%D0%B0%D1%8F_%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%B0)\"\u003Eтрекать во времени\u003C\u002Fa\u003E, распознавать действия и понимать, что на видео происходит. Именно этим вещам и будет посвящена вторая часть обзора.\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\nStay tuned!\u003Cbr\u002F\u003E\r\n\u003Cbr\u002F\u003E\r\n\u003Cdiv class=\"spoiler\"\u003E\u003Cb class=\"spoiler_title\"\u003EP. S.: Какое образование сейчас предлагает Физтех-школа ПМИ МФТИ?\u003C\u002Fb\u003E\u003Cdiv class=\"spoiler_text\"\u003EФПМИ МФТИ организует программу бакалавриата (кстати, теперь и \u003Ca href=\"http:\u002F\u002Fcs-mipt.ru\u002F\"\u003Eна английском\u003C\u002Fa\u003E, для иностранных студентов), однако сейчас активно развиваются и \u003Ca href=\"https:\u002F\u002Fmipt.ru\u002Feducation\u002Fdepartments\u002Ffpmi\u002Fmaster\u002F\"\u003Eмагистерские программы\u003C\u002Fa\u003E, причём как очные, так и \u003Ca href=\"http:\u002F\u002Fomscmipt.ru\u002F\"\u003Eонлайн\u003C\u002Fa\u003E. Со всем списком образовательных возможностей ФПМИ МФТИ можно ознакомиться \u003Ca href=\"https:\u002F\u002Fwww.dlschool.org\u002Ffpmi\"\u003Eздесь\u003C\u002Fa\u003E. Если вам интересно, буду рад обсудить конкретные программы в комментариях или личных сообщениях, а то ведь \u003Ca href=\"https:\u002F\u002Fmusic.yandex.ru\u002Falbum\u002F5237771\u002Ftrack\u002F40321941\"\u003EФизтех уже не тот\u003C\u002Fa\u003E (в хорошем смысле).\u003Cbr\u002F\u003E\r\n\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E\u003C\u002Fdiv\u003E","tags":[{"titleHtml":"ии"},{"titleHtml":"глубокое обучение"},{"titleHtml":"компьютерное зрение"},{"titleHtml":"обучение представлений"},{"titleHtml":"визуализация нейросетей"},{"titleHtml":"сверточные нейросети"},{"titleHtml":"computer vIsion"},{"titleHtml":"metric learning"},{"titleHtml":"нейронные сети"},{"titleHtml":"deep learning school"},{"titleHtml":"фпми"},{"titleHtml":"мфти"},{"titleHtml":"Физтех"}],"metadata":{"stylesUrls":[],"scriptUrls":[],"shareImageUrl":"https:\u002F\u002Fhabr.com\u002Fshare\u002Fpublication\u002F450732\u002F77baa3f666e996ccfb66cadf1cc3ee90\u002F","shareImageWidth":1200,"shareImageHeight":630,"vkShareImageUrl":"https:\u002F\u002Fhabr.com\u002Fshare\u002Fpublication\u002F450732\u002F77baa3f666e996ccfb66cadf1cc3ee90\u002F?format=vk","schemaJsonLd":"{\"@context\":\"http:\\\u002F\\\u002Fschema.org\",\"@type\":\"Article\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fmipt\\\u002Fblog\\\u002F450732\\\u002F\"},\"headline\":\"Вижу, значит существую: обзор Deep Learning в Computer Vision (часть 1)\",\"datePublished\":\"2019-05-21T14:31:04+03:00\",\"dateModified\":\"2020-01-03T22:12:43+03:00\",\"author\":{\"@type\":\"Person\",\"name\":\"Илья Захаркин\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Habr\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fa_\\\u002Flk\\\u002F9m\\\u002Fa_lk9mjkccjox-zccjrpfolmkmq.png\"}},\"description\":\"Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примера...\",\"url\":\"https:\\\u002F\\\u002Fhabr.com\\\u002Fru\\\u002Fcompany\\\u002Fmipt\\\u002Fblog\\\u002F450732\\\u002F#post-content-body\",\"about\":[\"c_mipt\",\"h_programming\",\"h_image_processing\",\"h_machine_learning\",\"h_artificial_intelligence\",\"f_develop\",\"f_popsci\"],\"image\":[\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fecb\\\u002F319\\\u002Fe06\\\u002Fecb319e06d692a5ea4f2a1343cf9c31d.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fgu\\\u002Fvu\\\u002Fo3\\\u002Fguvuo3vejwwjimlpcqiwgbpxldq.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F3x\\\u002Ftl\\\u002F-j\\\u002F3xtl-j0kmdt9ttlnakeka3kpj0u.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F967\\\u002F987\\\u002F50c\\\u002F96798750c04282d6514f994b8375edcb.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fdda\\\u002F997\\\u002F082\\\u002Fdda9970829bfb17bb2b118a08d519835.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fjz\\\u002F9k\\\u002F2o\\\u002Fjz9k2ovcurxg4zd_cj_kb20hs_0.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F4d1\\\u002Ffb8\\\u002F125\\\u002F4d1fb8125d4624b40993f441b42ac48d.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fwf\\\u002Fkw\\\u002Fla\\\u002Fwfkwlap8pltophsuh1ggkxgkii8.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F2d3\\\u002Ff3b\\\u002F178\\\u002F2d3f3b17818ae279e7a47d3c940e002f.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fhn\\\u002Fcw\\\u002Foc\\\u002Fhncwocoggiei8lkijpl8ihgbx_o.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fvg\\\u002Fvv\\\u002F4f\\\u002Fvgvv4f_ddwswudk1yvghxjl4rne.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fb4d\\\u002Fcfd\\\u002Fd13\\\u002Fb4dcfdd13f85affc79d876cf4bd3f4fd.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fcfa\\\u002F2bb\\\u002Fafa\\\u002Fcfa2bbafae96a5bd082ef25bae9d19af.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F60d\\\u002F62d\\\u002F670\\\u002F60d62d670999dcc7cbd726dde47905a0.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F113\\\u002F220\\\u002Fca0\\\u002F113220ca03176c5a99b82819076e0c8a.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fju\\\u002Fb7\\\u002Fi6\\\u002Fjub7i61z3oiairdg2q45x0l6loi.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fd56\\\u002F155\\\u002F0ee\\\u002Fd561550eec9f5badc4475392a584fe03.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F859\\\u002Fffd\\\u002F2d5\\\u002F859ffd2d56f231c5f9b802978a688c94.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F31f\\\u002F003\\\u002Fa47\\\u002F31f003a47c5dc5b5c5f75758d4d3689c.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F9a4\\\u002F3ea\\\u002F74b\\\u002F9a43ea74ba0b5595f257feb313756293.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fe90\\\u002F962\\\u002F25b\\\u002Fe9096225bb7d5799823737c960e19ad6.jpg\",\"https:\\\u002F\\\u002Fai2-s2-public.s3.amazonaws.com\\\u002Ffigures\\\u002F2017-08-08\\\u002F38211dc39e41273c0007889202c69f841e02248a\\\u002F2-Figure1-1.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F074\\\u002Fe15\\\u002Ff04\\\u002F074e15f04c8347ab32f98ba04aeceb6c.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F26c\\\u002F167\\\u002Fe3f\\\u002F26c167e3feb823e778b32278358053f9.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Ffb1\\\u002F3ca\\\u002Fd97\\\u002Ffb13cad97db640053bb2c53c12b0f4a7.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fc28\\\u002Fab9\\\u002F3c6\\\u002Fc28ab93c670c1e44258dc86064bb3a0c.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fbd4\\\u002F27e\\\u002F5e2\\\u002Fbd427e5e2943ebf58409e42538c4e131.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fz3\\\u002Fi4\\\u002Fb4\\\u002Fz3i4b4pxfnulxzfszysn_usqn_c.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Ffz\\\u002Fvm\\\u002Fym\\\u002Ffzvmymab57wgircssyfgxiaomvy.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Fe55\\\u002F809\\\u002F63f\\\u002Fe5580963fdfb998bfe2103f4cbf5aa8c.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fb-\\\u002F-k\\\u002Fiw\\\u002Fb--kiw7-vibdk8vpuzfxhbagkuu.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F5cd\\\u002Fb10\\\u002Fea8\\\u002F5cdb10ea8f19fe29432265e906640a90.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F887\\\u002Fd76\\\u002Feb4\\\u002F887d76eb431bcaf434ff70e2e0f2d4b0.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F55d\\\u002Fca5\\\u002F358\\\u002F55dca535836121c65546bc11e2d457c1.png\",\"http:\\\u002F\\\u002Fapi.ning.com\\\u002Ffiles\\\u002F1a5R6o7JsEHZ9j2SOd20XYu2GYExArt4Kr*0U07Z1JYbfSnF2ugTP7wmqMJn-l2auLHblJkG2QbtZcVqzScB81vPibkAjqBg\\\u002Ftransferlearning.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002Ffbc\\\u002F3ad\\\u002Ff28\\\u002Ffbc3adf280e28f7bb71246f50c1e8d9e.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fjh\\\u002F43\\\u002Fxs\\\u002Fjh43xsjjgxixbw8cmo1idxage5a.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F9s\\\u002Fpj\\\u002Fcm\\\u002F9spjcm6xbc2j2ip_wgri9wutjpi.jpeg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002F16\\\u002Fuh\\\u002Fn8\\\u002F16uhn8l_iahuy-bcv_e4vohx1je.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F208\\\u002F1b9\\\u002Fd34\\\u002F2081b9d346f74503302b8fd2c7265ef5.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Ft9\\\u002Fk3\\\u002Fzv\\\u002Ft9k3zvmuf30yzmcvlube_okh5ey.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Ff_\\\u002Fpe\\\u002Fcd\\\u002Ff_pecd2dvv5kbatdpj0nkk4aapm.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fca\\\u002Fpn\\\u002Fgr\\\u002Fcapngrtiskbeltdx0oq_wfntw0i.png\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fgetpro\\\u002Fhabr\\\u002Fpost_images\\\u002F456\\\u002F66e\\\u002F9c0\\\u002F45666e9c0608373c31452aeb6a197477.jpg\",\"https:\\\u002F\\\u002Fhabrastorage.org\\\u002Fwebt\\\u002Fue\\\u002F22\\\u002Fe1\\\u002Fue22e11md3zjexlxq3jxsf-kx18.jpeg\"]}","metaDescription":"Компьютерное зрение. Сейчас о нём много говорят, оно много где применяется и внедряется. И как-то давненько на Хабре не выходило обзорных статей по CV, с примерами архитектур и современными задачами....","mainImageUrl":null,"amp":false},"polls":[],"commentsEnabled":true,"rulesRemindEnabled":false,"votesEnabled":true,"status":"published","plannedPublishTime":null,"checked":null,"isEditorial":false}},"articlesIds":{},"isLoading":false,"pagesCount":{},"route":{},"reasonsList":null,"view":"cards","lastVisitedRoute":{},"ssrCommentsArticleIds":[""],"karma":{}},"authorContribution":{"authors":{}},"betaTest":{"currentAnnouncement":null,"announcements":{},"announcementCards":null,"announcementComments":{},"announcementCommentThreads":{},"announcementCommentingStatuses":{},"archivedList":[]},"authorStatistics":{"articleRefs":{},"articleIds":{},"pagesCount":{},"route":{},"viewsCount":[],"maxStatsCount":{}},"career":{"seoLandings":[],"hubs":""},"comments":{"articleComments":{},"searchCommentsResults":null,"previewComment":null,"pagesCount":null,"commentAccess":{},"scrollParents":{},"pageArticleComments":{"lastViewedComment":0,"postId":null,"lastCommentTimestamp":"","moderated":[],"moderatedIds":[],"commentRoute":""}},"companies":{"companyRefs":{"mipt":{"alias":"mipt","imageUrl":"\u002F\u002Fhabrastorage.org\u002Fgetpro\u002Fhabr\u002Fcompany\u002F70d\u002F544\u002Fdaf\u002F70d544daf4932b17afc9dd3efa0a9cf3.jpg","titleHtml":"Московский физико-технический институт (МФТИ)","descriptionHtml":null,"relatedData":null,"statistics":{"postsCount":75,"newsCount":0,"vacanciesCount":0,"employeesCount":14,"careerRating":null,"subscribersCount":666,"rating":57.62,"invest":null},"foundationDate":{"year":"1946","month":"11","day":"25"},"location":{"city":{"id":"447099","title":"Долгопрудный"},"region":{"id":"1885","title":"Москва и Московская обл."},"country":{"id":"168","title":"Россия"}},"siteUrl":"http:\u002F\u002Fmipt.ru","staffNumber":"1 001–5 000 человек","registrationDate":"2014-01-21T06:06:00+00:00","representativeUser":null,"contacts":[{"title":"Facebook","url":"https:\u002F\u002Ffacebook.com\u002FMIPT.rus"},{"title":"Twitter","url":"https:\u002F\u002Ftwitter.com\u002Fmiptru"},{"title":"ВКонтакте","url":"https:\u002F\u002Fvk.com\u002Fmiptru"},{"title":"Instagram","url":"https:\u002F\u002Finstagram.com\u002Fmipt.ru"}],"settings":{"analyticsSettings":[],"branding":null,"status":"expired"},"metadata":{"titleHtml":"Московский физико-технический институт (МФТИ), Долгопрудный -  с 25 ноября 1946 г.","title":"Московский физико-технический институт (МФТИ), Долгопрудный -  с 25 ноября 1946 г.","keywords":["Машинное обучение","Искусственный интеллект","Natural Language Processing","Спортивное программирование","Python"],"descriptionHtml":"75 статей от авторов компании Московский физико-технический институт (МФТИ)","description":"75 статей от авторов компании Московский физико-технический институт (МФТИ)"},"aDeskSettings":null,"careerAlias":null,"maxCustomTrackerLinks":0}},"companyIds":{},"companyTopIds":{},"pagesCount":{},"companyProfiles":{},"companiesCategories":[],"companiesCategoriesTotalCount":0,"companiesWidgets":{},"companiesWorkers":{},"companiesFans":{},"route":{},"isLoading":false,"companyWorkersLoading":false,"companyFansLoading":false,"vacancies":{}},"companiesContribution":{"hubs":{},"flows":{},"companyRefs":{}},"companyHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"conversation":{"messages":[],"respondent":null,"isLoadMore":false},"conversations":{"conversations":[],"unreadCount":0,"pagesCount":0,"isLoadMore":false},"desktopState":{"desktopFl":null,"desktopHl":null,"isChecked":false,"isLoginDemanded":false},"dfp":{"slotsDict":{}},"docs":{"menu":{},"articles":{},"mainMenu":[],"loading":{"main":false,"dropdown":false,"article":false}},"feature":{"isProbablyVisible":"true"},"flows":{"flows":[{"alias":"develop","id":1,"route":{"name":"FLOW_PAGE","params":{"flowName":"develop"}}},{"alias":"admin","id":6,"route":{"name":"FLOW_PAGE","params":{"flowName":"admin"}}},{"alias":"design","id":2,"route":{"name":"FLOW_PAGE","params":{"flowName":"design"}}},{"alias":"management","id":3,"route":{"name":"FLOW_PAGE","params":{"flowName":"management"}}},{"alias":"marketing","id":4,"route":{"name":"FLOW_PAGE","params":{"flowName":"marketing"}}},{"alias":"popsci","id":7,"route":{"name":"FLOW_PAGE","params":{"flowName":"popsci"}}}]},"global":{"isPwa":false,"device":"desktop","isHabrCom":true},"hubs":{"hubRefs":{},"hubIds":{},"pagesCount":{},"isLoading":false,"route":{}},"hubsBlock":{"hubRefs":{},"hubIds":{}},"i18n":{"fl":"ru","hl":"ru"},"info":{"infoPage":{},"isLoading":true},"location":{"urlStruct":{"protocol":null,"slashes":null,"auth":null,"host":null,"port":null,"hostname":null,"hash":null,"search":null,"query":{},"pathname":null,"path":null,"href":""},"searchQuery":null},"me":{"user":null,"ppgDemanded":false,"karmaResetInfo":{"canReincarnate":null,"wasReincarnated":null,"currentScore":null},"notes":null},"mostReadingList":{"mostReadingListIds":[],"mostReadingListRefs":null,"promoPost":null},"pinnedPost":{"pinnedPost":null},"ppa":{"articles":{},"card":null,"transactions":null,"totalTransactions":null,"isAccessible":null},"projectsBlocks":{"activeBlocks":{}},"pullRefresh":{"shouldRefresh":false},"sandbox":{"articleIds":[],"articleRefs":{},"pagesCount":null,"route":{},"lastVisitedRoute":{},"isLoading":false},"settingsOther":{"inputs":{"uiLang":{"errors":[],"ref":null,"value":""},"articlesLangEnglish":{"errors":[],"ref":null,"value":false},"articlesLangRussian":{"errors":[],"ref":null,"value":false},"agreement":{"errors":[],"ref":null,"value":false},"email":{"errors":[],"ref":null,"value":true},"digest":{"errors":[],"ref":null,"value":true}}},"similarList":{"similarListIds":[],"similarListRefs":null},"ssr":{"error":null,"isDataLoaded":false,"isDataLoading":false,"isHydrationFailed":false,"isServer":false},"userHubsContribution":{"contributionRefs":{"hubRefs":{},"hubIds":{}}},"userInvites":{"availableInvites":0,"usedInvitesIds":[],"usedInvitesRefs":{},"usedInvitesPagesCount":0,"unusedInvitesIds":[],"unusedInvitesRefs":{},"unusedInvitesPagesCount":0},"users":{"authorRefs":{},"authorIds":{},"pagesCount":{},"authorProfiles":{},"userHubs":{},"userInvitations":{},"authorFollowers":{},"authorFollowed":{},"karmaStats":[],"statistics":null,"isLoading":false,"authorFollowersLoading":false,"authorFollowedLoading":false,"userHubsLoading":false,"userInvitationsLoading":false,"route":{}},"viewport":{"prevScrollY":{},"scrollY":0,"width":0},"tracker":{"items":{},"pagesCache":{},"markedViewedSilently":{},"markedRead":{},"unreadCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null},"unviewedCounters":{"applications":null,"system":null,"mentions":null,"subscribers":null,"posts_and_comments":null}}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script>
<script src="https://assets.habr.com/habr-web/js/chunk-vendors.c2c3fc9a.js" defer></script><script src="https://assets.habr.com/habr-web/js/app.c0af73e7.js" defer></script>



    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    </script>
  
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(24049213, "init", {
      defer:true,
      trackLinks:true,
      accurateTrackBounce:true,
      webvisor:false,
    });
  </script>
  <noscript>
    <div>
      <img src="https://mc.yandex.ru/watch/24049213" style="position:absolute; left:-9999px;" alt="" />
    </div>
  </noscript>
  
    <script type="text/javascript">
      window.addEventListener('load', function () {
        setTimeout(() => {
          const img = new Image();
          img.src = 'https://vk.com/rtrg?p=VK-RTRG-421343-57vKE';
        }, 0);
      });
    </script>
  
</body>
</html>
